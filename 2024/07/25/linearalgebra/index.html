<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="Linear Algebra, Jay&#39;s Blog">
    <meta name="description" content="Linear AlgebraVector
vector
can be added together
can be multiplied by scalars


some examples of vector objects
Geometr">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>Linear Algebra | Jay&#39;s Blog</title>
    <link rel="icon" type="image/png" href="/favicon.png">
    


    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/css/matery.css">
<link rel="stylesheet" type="text/css" href="/css/my.css">
<link rel="stylesheet" type="text/css" href="/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/css/post.css">




    



    <script src="/libs/jquery/jquery-3.6.0.min.js"></script>

    <script src="https://sdk.jinrishici.com/v2/browser/jinrishici.js" charset="utf-8"></script>

<meta name="generator" content="Hexo 7.2.0">
<style>.github-emoji { position: relative; display: inline-block; width: 1.2em; min-height: 1.2em; overflow: hidden; vertical-align: top; color: transparent; }  .github-emoji > span { position: relative; z-index: 10; }  .github-emoji img, .github-emoji .fancybox { margin: 0 !important; padding: 0 !important; border: none !important; outline: none !important; text-decoration: none !important; user-select: none !important; cursor: auto !important; }  .github-emoji img { height: 1.2em !important; width: 1.2em !important; position: absolute !important; left: 50% !important; top: 50% !important; transform: translate(-50%, -50%) !important; user-select: none !important; cursor: auto !important; } .github-emoji-fallback { color: inherit; } .github-emoji-fallback img { opacity: 0 !important; }</style>
</head>


<body>
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/" class="waves-effect waves-light">
                    
                    <span class="logo-span">Jay&#39;s Blog</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>首页</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>标签</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>分类</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>归档</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/about" class="waves-effect waves-light">
      
      <i class="fas fa-user-circle" style="zoom: 0.6;"></i>
      
      <span>关于</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/galleries" class="waves-effect waves-light">
      
      <i class="fas fa-image" style="zoom: 0.6;"></i>
      
      <span>相册</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="搜索" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="深色/浅色模式" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <div class="logo-name">Jay&#39;s Blog</div>
        <div class="logo-desc">
            
            Jay&#39;s Blog
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			首页
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			标签
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			分类
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			归档
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/about" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-user-circle"></i>
			
			关于
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/galleries" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-image"></i>
			
			相册
		</a>
          
        </li>
        
        
    </ul>
</div>


        </div>

        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('/medias/featureimages/6.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">Linear Algebra</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- 文章内容详情 -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/tags/Mathematics/">
                                <span class="chip bg-color">Mathematics</span>
                            </a>
                        
                            <a href="/tags/%E7%AC%94%E8%AE%B0/">
                                <span class="chip bg-color">笔记</span>
                            </a>
                        
                            <a href="/tags/LinearAlgebra/">
                                <span class="chip bg-color">LinearAlgebra</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/categories/Mathematics/" class="post-category">
                                Mathematics
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>发布日期:&nbsp;&nbsp;
                    2024-07-25
                </div>
                

                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>文章字数:&nbsp;&nbsp;
                    6.2k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>阅读时长:&nbsp;&nbsp;
                    38 分
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>阅读次数:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        
        <!-- 是否加载使用自带的 prismjs. -->
        <link rel="stylesheet" href="/libs/prism/prism.min.css">
        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <h1 id="Linear-Algebra"><a href="#Linear-Algebra" class="headerlink" title="Linear Algebra"></a>Linear Algebra</h1><h2 id="Vector"><a href="#Vector" class="headerlink" title="Vector"></a>Vector</h2><ul>
<li><strong>vector</strong><ul>
<li>can be added together</li>
<li>can be multiplied by scalars</li>
</ul>
</li>
<li><strong>some examples of vector objects</strong><ul>
<li><strong><code>Geometric vectors</code></strong><ul>
<li>directed segments</li>
<li>direction and magnitude</li>
<li><img src="https://cdn.jsdelivr.net/gh/Eliauk-L/blog-img@img/img/202407251630723.png" alt="Geometric vectors"></li>
</ul>
</li>
<li><strong><code>Polynomials</code></strong><ul>
<li>can be added together, which results in another polynomial</li>
<li>can be multiplied by a scalar  λ, and the result is a polynomial as well</li>
<li><img src="https://cdn.jsdelivr.net/gh/Eliauk-L/blog-img@img/img/202407251631853.png" alt="Polynomials"></li>
</ul>
</li>
<li><strong><code>Audio signals</code></strong></li>
<li><strong>Elements of R^n^</strong>  (tuples of n real numbers)</li>
</ul>
</li>
</ul>
<h2 id="Systems-of-Linear-Equations"><a href="#Systems-of-Linear-Equations" class="headerlink" title="Systems of Linear Equations"></a>Systems of Linear Equations</h2><ul>
<li><p>Systems of Linear Equations</p>
<script type="math/tex; mode=display">
a_{11}x_1+\cdots+a_{1n}x_n=b_1
\\
\vdots
\\
a_{m1}x_1+\cdots+a_{mn}x_n=b_m</script></li>
<li><p>Geometric Interpretation of Systems of Linear Equations</p>
<ul>
<li><img src="https://cdn.jsdelivr.net/gh/Eliauk-L/blog-img@img/img/202407251631381.png" alt="Geometric Interpretation"></li>
<li>The solution space of a system of two linear equations with two variables can be geometrically interpreted as the intersection of two lines. Every linear equation represents a line.This intersection set can be a line (if the linear equations describe the same line), a point, or empty (when the lines are parallel).</li>
<li>For three variables, each linear equation determines a plane in three-dimensional space.The intersection set can a solution set that is a plane, a line, a point or empty (when the planes have no common intersection).</li>
</ul>
</li>
<li><p>Matrix representation of systems of linear equations</p>
<script type="math/tex; mode=display">
\left[ \begin{array}{c}
        a_{11}\\
        \vdots\\
        a_{m1}\\
\end{array} \right] x_1+\left[ \begin{array}{c}
        a_{12}\\
        \vdots\\
        a_{m2}\\
\end{array} \right] x_2+\cdots+\left[ \begin{array}{c}
        a_{1n}\\
        \vdots\\
        a_{mn}\\
\end{array} \right] x_n=\left[ \begin{array}{c}
        b_{1}\\
        \vdots\\
        b_{m}\\
\end{array} \right] \Leftrightarrow \left[ \begin{matrix}
        a_{11}&        \cdots&        a_{1n}\\
        \vdots&        &            \vdots\\
        a_{m1}&        \cdots&        a_{mn}\\
\end{matrix} \right] \left[ \begin{array}{c}
        b_1\\
        \vdots\\
        b_m\\
\end{array}\right]</script></li>
</ul>
<h2 id="Matrices"><a href="#Matrices" class="headerlink" title="Matrices"></a>Matrices</h2><ul>
<li><p><strong>Matrices</strong></p>
<ul>
<li>represent systems of linear equations</li>
<li>represent linear functions (linear mappings)</li>
</ul>
</li>
<li><p><strong>a real-valued (m,n) matrix A</strong></p>
<script type="math/tex; mode=display">
\left[ \begin{matrix}
        a_{11}&        a_{12}&        \cdots&        a_{1n}\\
        a_{21}&        a_{22}&        \cdots&        a_{2n}\\
        \vdots&        \vdots&        &            \vdots\\
        a_{m1}&        a_{m2}&        \cdots&        a_{mn}\\
\end{matrix} \right]</script></li>
<li><p><strong>R^m×n^ is the set of all real-valued (m,n)-matrices. A ∈ R^m×n^ can be</strong><br><strong>equivalently represented as a ∈ R^mn^ by stacking all n columns of the</strong><br><strong>matrix into a long vector;</strong><br><img src="https://cdn.jsdelivr.net/gh/Eliauk-L/blog-img@img/img/202407251632155.png" alt="Equivalently represent"></p>
</li>
<li><p><strong>Matrix Addition</strong></p>
<script type="math/tex; mode=display">
A+B:=\left[\begin{matrix}
a_{11}+b_{11}& \cdots& a_{1n}+b_{1n}\\
\vdots& & \vdots\\
a_{m1}+b_{m1}& \cdots& a_{mn}+b_{mn}
\end{matrix}\right]\in \mathbb R^{m\times n}</script></li>
<li><p><strong>Matrix Multiplication</strong>: <strong><code>dot product</code></strong></p>
<script type="math/tex; mode=display">
C=AB\in R^{m\times k}\Leftrightarrow c_{ij}=\sum_{l=1}^n{a_{il}b_{lj}},i=1,\cdots,m, j=1,\cdots,k</script><blockquote>
<p>Commonly, the dot product between two vectors a, b is denoted by a^⊤^b or ⟨a, b⟩.</p>
<p><strong>Hadamard product</strong>: an element-wise multiplication <strong>c~ij~=a~ij~b~ij~</strong></p>
</blockquote>
<ul>
<li><strong>not commutative</strong> <code>AB≠BA</code></li>
</ul>
</li>
<li><p><strong>Identity Matrix</strong></p>
<p><img src="https://cdn.jsdelivr.net/gh/Eliauk-L/blog-img@img/img/202407251635239.png" alt="Identity Matrix"></p>
</li>
<li><p><strong>Some properties of matrices</strong></p>
<ul>
<li><strong><code>Associativity</code></strong><br><img src="https://cdn.jsdelivr.net/gh/Eliauk-L/blog-img@img/img/202407251635359.png" alt="Associativity"></li>
<li><strong><code>Distributivity</code></strong><br><img src="https://cdn.jsdelivr.net/gh/Eliauk-L/blog-img@img/img/202407251635397.png" alt="Distributivity"></li>
<li><strong><code>Multiplication with the identity matrix</code></strong><br><img src="https://cdn.jsdelivr.net/gh/Eliauk-L/blog-img@img/img/202407251635823.png" alt="Multiplication with the identity matrix"></li>
</ul>
</li>
<li><p><strong>Inverse</strong></p>
<ul>
<li><p>Consider a square matrix $A\in R^{n\times n}$. Let matrix $B\in R^{n\times n}$ have the property that $AB=I_{n}=BA$. $B$ is called the inverse of $A$ and denoted by $A^{−1}$.</p>
</li>
<li><p>If the inverse <strong>does</strong> exist, A is called <strong>regular/invertible/nonsingular</strong>, otherwise <strong>singular/noninvertible</strong>.</p>
</li>
<li><p>When the matrix inverse exists, it is unique.</p>
</li>
<li><p>Existence of the Inverse of a 2 × 2-matrix: $a_{11}a_{22}-a_{12}a_{21}\ne0$</p>
<script type="math/tex; mode=display">
A:=\left[ \begin{matrix}
        a_{11}&        a_{12} \\
        a_{21}&        a_{22}
\end{matrix} \right] \in R^{2\times 2}\\
A^{-1}:=\frac{1}{a_{11}a_{22}-a_{12}a_{21}} \left[ \begin{matrix}
        a_{22}&        -a_{12} \\
        -a_{21}&        a_{11}
\end{matrix} \right]</script></li>
</ul>
</li>
<li><p><strong>Transpose</strong></p>
<ul>
<li>For $A ∈ R^{m×n}$ the matrix $B ∈ R^{n×m}$ with $b_{ij} = a_{ji}$ is called the transpose of $A$. We write $B = A^⊤$</li>
<li>$A^⊤$ can be obtained by <strong>writing the columns of $A$ as the rows of $A^⊤$</strong></li>
</ul>
</li>
<li><p><strong>Properties of inverses and transposes</strong></p>
<script type="math/tex; mode=display">
AA^{-1}= I =A^{-1}A \\
(AB)^{-1} =  B^{-1} A^{-1} \\
(A+B)^{-1} \ne A^{-1} + B^{-1} \\
(A^{T})^{T} = A \\ 
(A+B)^{T} = A^{T} + B^{T} \\
(AB)^{T} = B^{T} A^{T}</script></li>
<li><p><strong>Symmetric Matrix</strong>:  $A = A^⊤,A ∈ R^{n×n}$.</p>
<ul>
<li>only <strong>(n, n)-matrices</strong>(square matrices) can be symmetric</li>
<li>if $A$ is invertible, then so is $A^⊤$, and $(A^{−1})^{⊤} = (A^⊤)^{−1} =: A^{−⊤}$.</li>
<li>The sum of symmetric matrices is always symmetric but not product.</li>
</ul>
</li>
<li><p><strong>Matrices multiplication by a Scalar</strong>: $\lambda A = K, K_{ij} = \lambda a_{ij},\lambda \in R,A \in R^{m\times n}$</p>
<ul>
<li><p>For $\lambda,\psi \in R$</p>
</li>
<li><p><strong><code>Associativity</code></strong></p>
<script type="math/tex; mode=display">
(\lambda \psi)C = \lambda(\psi C),C\in R^{m\times n}\\
\lambda(BC) = (\lambda B)C = B(\lambda C) = (BC)\lambda,B\in R^{m\times n},C\in R^{n\times k}\\
(\lambda C)^T = C^T\lambda^T=C^T\lambda=\lambda C^T</script></li>
<li><p><strong><code>Distributivity</code></strong></p>
<script type="math/tex; mode=display">
(\lambda + \psi)C = \lambda C + \psi C,C\in R^{m\times n}\\
\lambda(B+C) = \lambda B + \lambda C,\quad B,C\in R^{m\times n}\\</script></li>
</ul>
</li>
</ul>
<h2 id="Solving-Systems-of-Linear-Equations"><a href="#Solving-Systems-of-Linear-Equations" class="headerlink" title="Solving Systems of Linear Equations"></a>Solving Systems of Linear Equations</h2><ul>
<li><strong>The particular solution and general solution</strong></li>
<li>Three steps<ol>
<li>Find a particular solution to $Ax = b$.</li>
<li>Find all solutions to $Ax = 0$.</li>
<li>Combine the solutions from steps 1. and 2. to the general solution.</li>
</ol>
</li>
<li>Neither the general nor the particular solution is unique</li>
<li><strong>Gaussian elimination</strong>: transforme any system of linear equations into the particularly simple form. Key to Gaussian elimination are <strong>elementary transformations</strong>.</li>
<li><strong>Elementary transformations</strong><ul>
<li><strong><code>Exchange</code></strong> of two equations (rows in the matrix representing the system of equations)</li>
<li><strong><code>Multiplication</code></strong> of an equation (row) with a constant $λ ∈ R \backslash \{0\}$</li>
<li><strong><code>Addition</code></strong> of two equations (rows)</li>
</ul>
</li>
</ul>
<blockquote>
<p><strong>e.g. </strong>  For  <strong>a ∈ R</strong> </p>
<p><img src="https://cdn.jsdelivr.net/gh/Eliauk-L/blog-img@img/img/202407251635152.png" alt="System of linear equations"></p>
<p>the augmented matrix (in the form$\left [<br>\begin{array}{c|c}<br>\begin{matrix}<br>\mathbf A<br>\end{matrix}&amp;<br>\begin{matrix}<br>\mathbf b<br>\end{matrix}<br>\end{array}<br>\right ]$):</p>
<p><img src="https://cdn.jsdelivr.net/gh/Eliauk-L/blog-img@img/img/202407251636698.png" alt="the augmented matrix"></p>
<p>Swapping Rows 1 and 3 leads to</p>
<p><img src="https://cdn.jsdelivr.net/gh/Eliauk-L/blog-img@img/img/202407251636098.png" alt="Elementary transformations01"></p>
<p><img src="https://cdn.jsdelivr.net/gh/Eliauk-L/blog-img@img/img/202407251636764.png" alt="Elementary transformations02"></p>
<p>This (augmented) matrix is in a convenient form, the <strong>row-echelon form(REF)</strong></p>
<p>Reverting this compact notation back into the explicit notation with the variables we seek</p>
<p><img src="https://cdn.jsdelivr.net/gh/Eliauk-L/blog-img@img/img/202407251636233.png" alt="REF"></p>
<p>Only for a = −1 this system can be solved.</p>
<p>A particular solution is</p>
<script type="math/tex; mode=display">
\left[\begin{matrix}
        x_{1} \\
        x_{2} \\
     x_{3} \\
     x_{4} \\
     x_{5} \\
\end{matrix} \right] = 
\left[\begin{matrix}
        2 \\
        0 \\
     -1 \\
     1 \\
     0 \\
\end{matrix} \right]</script><p>The general solution</p>
<p><img src="https://cdn.jsdelivr.net/gh/Eliauk-L/blog-img@img/img/202407251637135.png" alt="general solution"></p>
</blockquote>
<ul>
<li><p><strong>Pivots and Staircase Structure</strong>: The leading coefficient of a row (first nonzero number from the left) is called the pivot and is always strictly to the right of the pivot of the row above it</p>
</li>
<li><p><strong>Row-Echelon Form</strong></p>
<ul>
<li>All rows that contain <strong>only zeros</strong> are at the <strong>bottom</strong> of the matrix; all rows that contain at <strong>least one nonzero</strong> element are on <strong>top</strong> of rows that contain only zeros.</li>
<li>Looking at nonzero rows only, <strong>the first nonzero number</strong> from the left (also called the pivot or the leading coefficient) is always strictly to the right of the pivot of the row above it.</li>
</ul>
</li>
<li><p><strong>Basic and Free Variables</strong></p>
<ul>
<li>The variables corresponding to the <strong>pivots</strong> in the row-echelon form are called <strong>basic variables</strong> and the other variables are <strong>free variables</strong>. </li>
<li>In the above example, x1, x3, x4 are basic free variable, whereas x2, x5 are free variables.</li>
</ul>
</li>
<li><p><strong>Obtaining a Particular Solution</strong></p>
<ul>
<li><p>We express the right-hand side of the equation system using the pivot columns, such that $b=\sum_{i=1}^P{\lambda_ip_i}$ ,where $p_i,i=1,\cdots,P$​​ are <strong>the pivot columns</strong>.The $λ_i$ are determined easiest if we start with the rightmost pivot column and work our way to the left</p>
</li>
<li><p>In the previous example, we would try to find $λ_1, λ_2, λ_3$ so that</p>
<script type="math/tex; mode=display">
\lambda_1 \left[\begin{matrix} 
1 \\
0 \\
0 \\
0 \\
\end{matrix}\right] 
+
\lambda_2 \left[\begin{matrix} 
1 \\
1 \\
0 \\
0 \\
\end{matrix}\right] 
+
\lambda_3 \left[\begin{matrix} 
-1 \\
-1 \\
1 \\
0 \\
\end{matrix}\right] 
=
\left[\begin{matrix} 
0 \\
-2 \\
1 \\
0 \\
\end{matrix}\right]</script><p>$\lambda_1=2,\lambda_2=-1,\lambda_3=1$ And we must not forget <strong>the non-pivot columns</strong><br><strong>for which we set the coefficients implicitly to 0.</strong><br>Therefore, we get the particular solution $\mathbf x=[2,0,-1,1,0]^T$</p>
</li>
</ul>
</li>
<li><p><strong>Reduced Row Echelon Form</strong></p>
<ul>
<li>It is in <strong>row-echelon form</strong>.</li>
<li><strong>Every pivot is 1</strong>.</li>
<li>The pivot is the <strong>only nonzero</strong> entry in its column.</li>
</ul>
</li>
<li><p>The reduced row-echelon form allows us to determine the general solution of a system of linear equations in a straightforward way.</p>
<script type="math/tex; mode=display">
A=\left[\begin{matrix}
\mathbf 1& 3& 0& 0&    3 \\
0& 0& \mathbf 1& 0&    9 \\
0& 0& 0& \mathbf 1&    -4 \\
\end{matrix}\right]</script><ul>
<li>The key idea for finding the solutions of $Ax = 0$ is to look at <strong>the non-pivot columns</strong>, which we will need to express as <strong>a (linear) combination of the pivot columns</strong>.<br><img src="https://cdn.jsdelivr.net/gh/Eliauk-L/blog-img@img/img/202407251637666.png" alt="reduced row-echelon "><script type="math/tex; mode=display">
\left\{x\in R^5:x=\lambda_1 
\left[\begin{matrix}
3  \\
-1  \\
0 \\
0 \\
0 \\
\end{matrix}\right]
+ \lambda_2
\left[\begin{matrix}
3  \\
0  \\
9 \\
-4 \\
-1 \\
\end{matrix}\right],\lambda_1,\lambda_2\in R
\right\}</script></li>
</ul>
</li>
<li><p>A practical trick for reading out the solutions $x$ of a homogeneous system of linear equations $Ax = 0$, where $A ∈ R^{k×n}, x ∈ R^{n}$.</p>
<ul>
<li><p>We assume that $A$ is in reduced row-echelon form without anyrows that just contain zeros<img src="https://cdn.jsdelivr.net/gh/Eliauk-L/blog-img@img/img/202407251638273.png" alt="reduced row-echelon form"></p>
<p>where ∗ can be an arbitrary real number, with the constraints that the first nonzero entry per row must be 1 and all other entries in the corresponding column must be 0.</p>
</li>
<li><p>The columns $j_1, . . . , j_k$ with the pivots (marked in <strong>bold</strong>) are the standard unit vectors $e_1, . . . , e_k ∈ R^k$. We extend this matrix to an $n × n$-matrix $\tilde A$ by adding $n − k$ rows of the form</p>
<script type="math/tex; mode=display">
\left[\begin{matrix} 
0& \cdots& 0& -1& 0& \cdots& 0
\end{matrix} \right]</script><p>so that the diagonal of the augmented matrix $\tilde A$ contains either 1 or −1.</p>
</li>
<li><p><strong>The columns of $\tilde A$ that contain the −1 as pivots are solutions of the homogeneous equation system $Ax = 0$.</strong></p>
</li>
</ul>
<blockquote>
<p>e.g. reduced REF</p>
<script type="math/tex; mode=display">
A=\left[\begin{matrix}
1& 3& 0& 0&    3 \\
0& 0& 1& 0&    9 \\
0& 0& 0& 1&    -4 \\
\end{matrix}\right]</script><p>We now augment this matrix to a 5 × 5 matrix by <strong>adding rows of the form at the places where the pivots on the diagonal are missing.</strong></p>
<script type="math/tex; mode=display">
\tilde A = 
\left[\begin{matrix}
1& 3& 0& 0&    3 \\
\mathbf0& \mathbf{-1}& \mathbf0& \mathbf0& \mathbf0 \\
0& 0& 1& 0&    9 \\
0& 0& 0& 1&    -4 \\
\mathbf0& \mathbf0& \mathbf0& \mathbf0&    \mathbf{-1} \\
\end{matrix}\right]</script><p>We can immediately read out the solutions of $Ax = 0$ by taking the columns of $\tilde A$, which contain −1 on the diagonal:</p>
<script type="math/tex; mode=display">
\left\{x\in R^5:x=\lambda_1 
\left[\begin{matrix}
3  \\
-1  \\
0 \\
0 \\
0 \\
\end{matrix}\right]
+ \lambda_2
\left[\begin{matrix}
3  \\
0  \\
9 \\
-4 \\
-1 \\
\end{matrix}\right],\lambda_1,\lambda_2\in R
\right\}</script></blockquote>
</li>
<li><p><strong>Calculating the Inverse</strong></p>
<ul>
<li><p>We use the augmented matrix notation for a compact representation of this set of systems of linear equations and obtain</p>
<script type="math/tex; mode=display">
\left[
\begin{array}{c|c}
A& I_n
\end{array}
\right] 
\rightarrow \cdots \rightarrow
\left[
\begin{array}{c|c}
I_n & A^{-1}
\end{array}
\right]</script></li>
<li><p>If we bring the augmented equation system into reduced row-echelon form, we can read out the inverse on the right-hand side of the equation system.</p>
</li>
</ul>
<blockquote>
<p>e.g. </p>
<script type="math/tex; mode=display">
A=\left[\begin{matrix}
1& 0& 2& 0 \\
1& 1& 0& 0 \\
1& 2& 0& 1 \\
1& 1& 1& 1 \\
\end{matrix}\right]</script><p>the augmented matrix</p>
<script type="math/tex; mode=display">
A=\left[\begin{matrix}
1& 0& 2& 0& 1& 0& 0& 0\\
1& 1& 0& 0& 0& 1& 0& 0\\
1& 2& 0& 1& 0& 0& 1& 0\\
1& 1& 1& 1& 0& 0& 0& 1\\
\end{matrix}\right]</script><p>use Gaussian elimination to bring it into reduced row-echelon form</p>
<script type="math/tex; mode=display">
\left[
\begin{array}{c|c}
I_n & A^{-1}
\end{array}
\right] = \left[\begin{array}{c|c} 
\begin{matrix}
1& 0& 0& 0& \\
0& 1& 0& 0& \\
0& 0& 1& 0& \\
0& 0& 0& 1& \\
\end{matrix} & 
\begin{matrix}
-1& 0& -2& 2\\
1& -1& 2& -2\\
1& -1& 1& -1\\
-1& 0& -1& 2\\
\end{matrix}
\end{array}\right]</script></blockquote>
</li>
<li><p>If A is <strong>a square matrix and invertible</strong>, the solution of $Ax = b$ is given as $x = A^{−1}b$. Otherwise, under mild assumptions (i.e., A needs to have linearly independent columns) we can use the transformation</p>
<script type="math/tex; mode=display">
Ax=b
\Longleftrightarrow
A^TAx=A^Tb
\Longleftrightarrow
x=(A^TA)^{-1}A^Tb</script><p>the Moore-Penrosepseudo-inverse：$(A^TA)^{-1}A^T$</p>
</li>
<li><p>Systems of many linear equations are solved indirectly, by either stationary iterative methods, such as <strong>the Richardson method, the Jacobi method, the Gauß-Seidel method, and the successive over-relaxation method, or Krylov subspace methods, such as conjugate gradients, generalized minimal residual, or biconjugate gradients.</strong></p>
</li>
<li><p>Let $x_∗$ be a solution of $Ax = b$. The key idea of these iterative methods is to set up an iteration of the form</p>
<script type="math/tex; mode=display">
x^{(k+1)} = Cx^{(k)} + d</script><p>for suitable $C$ and $d$ that reduces the residual error $∥x^{(k+1)}−x_∗∥$ in every iteration and converges to $x_∗$.</p>
</li>
</ul>
<h2 id="Vector-Spaces"><a href="#Vector-Spaces" class="headerlink" title="Vector Spaces"></a>Vector Spaces</h2><ul>
<li><p><strong>Group</strong>: Consider a set $\mathcal G$ and an operation $\otimes : \mathcal G\times \mathcal G → \mathcal G$ defined on $\mathcal G$. Then $G := (\mathcal G, ⊗)$ is called a group if the following hold:</p>
<ul>
<li><strong>Closure</strong> of $\mathcal G$ under ⊗: $∀x, y ∈ \mathcal G : x ⊗ y ∈ \mathcal G$</li>
<li><strong>Associativity</strong>: $∀x, y, z ∈ \mathcal G : (x ⊗ y) ⊗ z = x ⊗ (y ⊗ z)$</li>
<li><strong>Neutral element</strong>: $∃e ∈ \mathcal G \;∀x ∈ \mathcal G : x ⊗ e = x \quad and \quad e ⊗ x = x$</li>
<li><strong>Inverse element</strong>: $∀x ∈ \mathcal G \;∃y ∈ \mathcal G : x ⊗ y = e \quad and \quad y ⊗ x = e$, where $e$ is the neutral element. We often write $x^{−1}$ to denote the inverse element of $x$. (The inverse element is defined with respect to the operation ⊗ and does not necessarily mean $\frac{1}{x}$)</li>
</ul>
</li>
<li><p>If additionally $∀x, y ∈ \mathcal G : x ⊗ y = y ⊗ x$, then $G = (\mathcal G, ⊗)$ is an Abelian group (commutative).</p>
</li>
<li><p>Some examples of sets with associated operations</p>
<ul>
<li><strong>$(\mathbb Z, +)$ is an Abelian group.</strong></li>
<li>$(\mathbb N_0, +)$ is not a group: Although $(\mathbb N_0, +)$ possesses a neutral element(0), the inverse elements are missing.  $\mathbb N_0 := \mathbb N∪ \{0\}$</li>
<li>$(\mathbb Z, ·)$ is not a group: Although $(\mathbb Z, ·)$ contains a neutral element (1), the inverse elements for any $z ∈ \mathbb Z, z \ne ±1$, are missing.</li>
<li>$(\mathbb R, ·)$ is not a group since 0 does not possess an inverse element</li>
<li><strong>$(\mathbb R\backslash\{0\}, ·)$ is Abelian</strong></li>
<li>$(\mathbb R^n, +), (\mathbb Z^n, +), n ∈ \mathbb N$ <strong>are Abelian</strong> if $+$ is defined componentwise, i.e.,<br>$(x_1, · · · , x_n) + (y_1, · · · , y_n) = (x_1 + y_1, · · · , x_n + y_n)$<br>Then, $(x_1, · · · , x_n)^{−1}:= (−x_1, · · · , −x_n)$ is the inverse element and $e = (0, · · · , 0) $is the neutral element.</li>
<li><strong>$(\mathbb R^{m×n}, +)$, the set of $m× n$-matrices is Abelian</strong> (with componentwise addition)</li>
<li>$(\mathbb R^{n×n}, ·)$，the set of n×n-matrices with matrix multiplication<ul>
<li>Closure and associativity follow directly from the definition of matrix multiplication.</li>
<li>Neutral element: The <strong>identity matrix</strong> $I_n$ is the neutral element with respect to matrix multiplication “·” in $(\mathbb R^{n×n}, ·)$.</li>
<li>Inverse element: If the inverse exists ($A$ is regular), then $A^{−1}$ is the inverse element of $A ∈ R^{n×n}$, and in exactly this case <strong>$(R^{n×n}, ·)$ is a group, called the general linear group $GL(n,\mathbb R)$.</strong> Since matrix multiplication is not commutative, <strong>the group is not Abelian.</strong></li>
</ul>
</li>
</ul>
</li>
<li><p><strong>Vector Space</strong>: A real-valued vector space $V = (\nu, +, ·)$ is a set $\nu$ with two operations</p>
<script type="math/tex; mode=display">
+:\nu \times \nu \rightarrow \nu \\
\cdot:\mathbb R \times \nu \rightarrow \nu</script><p>where</p>
<ol>
<li><p>$(\nu, +)$ is an Abelian group</p>
</li>
<li><p>Distributivity</p>
<script type="math/tex; mode=display">
∀λ ∈ \mathbb R, x, y ∈ \nu : λ · (x + y) = λ · x + λ · y \\
∀λ, ψ ∈ \mathbb R, x ∈ \nu : (λ + ψ) · x = λ · x + ψ · x</script></li>
<li><p>Associativity (outer operation): $∀λ, ψ ∈ \mathbb R, x ∈ \nu : λ·(ψ·x) = (λψ)·x$</p>
</li>
<li><p>Neutral element with respect to the outer operation: $∀x ∈ \nu : 1·x = x$</p>
</li>
</ol>
</li>
<li><p>Some important examples</p>
<ul>
<li>$V = \mathbb R^n, n ∈ \mathbb N$ is a vector space with operations defined as follows<ul>
<li>Addition: $x+y = (x_1, . . . , x_n)+(y_1, . . . , y_n) = (x_1+y_1, . . . , x_n+y_n)$ for all $x, y ∈ \mathbb R^n$</li>
<li>Multiplication by scalars: $λx = λ(x_1, . . . , x_n) = (λx_1, . . . , λx_n)$ for all $λ ∈ \mathbb R, x ∈ \mathbb R^{n}$</li>
</ul>
</li>
<li>$V = \mathbb C$, with the standard definition of addition of complex numbers.</li>
</ul>
</li>
<li><p>The elements $x ∈ V$ are called vectors. The neutral element of $(\nu, +)$ is the zero vector $\mathbf 0 = [0, . . . , 0]^⊤$, and the inner operation $+$ is called vector addition. The elements $λ ∈ R $are called scalars and the outer operation $·$ is a multiplication by scalars.</p>
</li>
<li><p>A “vector multiplication” $ab, a, b ∈ \mathbb R^n$, is not defined. We could define an element-wise multiplication, such that $c = ab$ with $c_j = a_jb_j$. Only the following multiplications for vectors are defined: $ab^⊤ ∈ \mathbb R^{n×n}$ (<strong>outer  product</strong>), $a^⊤b ∈ \mathbb R$ (<strong>inner/scalar/dot product</strong>).</p>
</li>
<li><p>By default, we write $x$ to denote a column vector, and a row vector is denoted by $x^⊤$, the transpose of $x$</p>
</li>
<li><p><strong>Vector Subspace</strong>： Let $V = (\nu, +, ·)$ be a vector space and $u ⊆ \nu, u \ne ∅$. Then $U = (u, +, ·)$ is called <strong>vector subspace</strong> of $V$ (or linear subspace) if U is a vector space with the vector space operations + and · restricted to $u×u$ and $\R×u$. We write $U ⊆ V$ to denote a subspace U of V.   U naturally inherits many properties directly from V</p>
<ul>
<li>For every vector space $V$, the trivial subspaces are $V$ itself and $\{0\}$.</li>
<li>Not all subsets of $\mathbb R^2$ are subspaces. In A and C, the closure property is violated; B does not contain 0. Only D is a subspace.<br><img src="https://cdn.jsdelivr.net/gh/Eliauk-L/blog-img@img/img/202407251638848.png" alt="Vector Subspace"></li>
<li>The solution set of a homogeneous system of linear equations $Ax = 0$ with $n $unknowns $x = [x_1, . . . , x_n]^⊤$ is a subspace of $\mathbb R^n$. And the solution of an inhomogeneous system of linear equations $Ax = b, b \ne 0$ is not a subspace of $\mathbb R^n$.</li>
<li>The intersection of arbitrarily many subspaces is a subspace itself.</li>
<li>Every subspace $U ⊆ (\mathbb R^n, +, ·)$ is the solution space of a homogeneous system of linear equations $Ax = 0$ for $x ∈ \mathbb R^n$.</li>
</ul>
</li>
</ul>
<h2 id="Linear-Independence"><a href="#Linear-Independence" class="headerlink" title="Linear Independence"></a>Linear Independence</h2><ul>
<li><p><strong>Linear Combination</strong>: Consider a vector space $V$ and a finite number of vectors $x_1, . . . , x_k ∈ V$. Then, every $v ∈ V$ of the form</p>
<script type="math/tex; mode=display">
v = \lambda_1x_1 + \cdots + \lambda_kx_k = \sum_{i=1}^k{\lambda_ix_i}\in V</script><p>with $λ_1, . . . , λ_k ∈ \mathbb R$ is <strong>a linear combination of the vectors</strong> $x_1, . . . , x_k$.</p>
</li>
<li><p>The <strong>0-vector</strong> can always be written as the linear combination of $k$ vectors $x_1, . . . , x_k $because $0 =\sum_{i=1}^k{0x_i}$ is always true.</p>
</li>
<li><p><strong>Linear (In)dependence</strong>: A vector space $V$ with $k ∈ \mathbb N$ and $x_1, . . . , x_k ∈ V$. If there is a non-trivial linear combination, such that $0 = \sum_{i=1}^k{\lambda_ix_i}$ with <strong>at least one</strong> $\mathbf λ_i \ne 0$, the vectors $x_1, . . . , x_k$ are <strong>linearly dependent</strong>. If only <strong>the trivial solution</strong> exists, i.e.,$λ_1 = . . . = λ_k = 0$ the vectors $x_1, . . . , x_k$ are <strong>linearly independent</strong>.</p>
<ul>
<li>$k$ vectors are either linearly dependent or linearly independent.</li>
<li>If at least one of the vectors $x_1, . . . , x_k$ is $0$ then they are linearly dependent. The same holds if two vectors are identical.</li>
<li>The vectors $\{x_1, . . . , x_k : x_i \ne 0, i = 1, . . . , k\}, k ⩾ 2$, are linearly dependent if and only if <strong>(at least) one of them is a linear combination of the others.</strong> In particular, if one vector is a multiple of another vector, i.e., $x_i = λx_j, λ ∈ \mathbb R$ then the set $\{x_1, . . . , x_k : x_i \ne 0, i = 1, . . . , k\}$ is linearly dependent.</li>
<li>Use Gaussian elimination: Write all vectors as columns of a matrix $A$ and perform Gaussian elimination until the matrix is in row echelon form (the reduced row-echelon form is unnecessary here):<ul>
<li>The pivot columns indicate the vectors, which are linearly independent of the vectors on the left. Note that there is an ordering of vectors when the matrix is built.</li>
<li>The non-pivot columns can be expressed as linear combinations of the pivot columns on their left.</li>
<li>All column vectors are linearly independent if and only if <strong>all columns are pivot columns</strong>. If there is <strong>at least one non-pivot column</strong>, the columns (and, therefore, the corresponding vectors) are linearly dependent.</li>
</ul>
</li>
</ul>
</li>
<li><p>In a vector space $V$, $m$ linear combinations of $k$ vectors $x_1, . . . , x_k$ are linearly dependent if $m &gt; k$.</p>
</li>
</ul>
<h2 id="Basis-and-Rank"><a href="#Basis-and-Rank" class="headerlink" title="Basis and Rank"></a>Basis and Rank</h2><ul>
<li><p><strong>Generating Set and Span</strong>: Consider a vector space $V =(\nu, +, ·)$ and set of vectors $A = {x_1, . . . , x_k} ⊆ \nu$. If every vector $v ∈\nu$ can be expressed as a <em>linear combination</em> of $x_1, . . . , x_k$, $A$ is called a <strong>generating set</strong> of $V$. The set of all linear combinations of vectors in A is called the <strong>span</strong> of $A$. If $A$ spans the vector space $V$, we write $V = span[A]$ or $V = span[x_1, . . . , x_k]$.</p>
</li>
<li><p><strong>Basis</strong>: Consider a vector space $V = (\nu, +, ·)$ and $A ⊆ \nu$. A generating set $A$ of $V$ is called <strong>minimal</strong> if there exists <strong>no smaller set $\tilde A⊊ A ⊆ \nu$ that spans $V$.</strong> <strong>Every linearly independent generating set of $V$ is minimal and is called a basis of $V$.</strong></p>
</li>
<li><p>Let $V = (\nu, +, ·)$ be a vector space and $\mathcal B ⊆ \nu, \mathcal B \ne ∅$. Then, the following statements are equivalent</p>
<ul>
<li>$\mathcal B$ is a <strong>basis</strong> of $V$.</li>
<li>$\mathcal B$ is a <strong>minimal generating set</strong>.</li>
<li>$\mathcal B$ is a <strong>maximal linearly independent</strong> set of vectors in $V$, i.e., adding any other vector to this set will make it linearly dependent.</li>
<li>Every vector $x ∈ V$ is a linear combination of vectors from $\mathcal B$, and every linear combination is unique.</li>
</ul>
</li>
<li><p><strong>Every vector space $V$ possesses a basis</strong> $\mathcal B$. And there can be many bases of a vector space $V$, i.e., there is no unique basis. However, all bases possess <strong>the same number of elements, the basis vectors.</strong></p>
</li>
<li><p>Consider finite-dimensional vector spaces $V$, the <strong><em>dimension</em></strong> of $V$ is <strong>the number of basis vectors</strong> of $V$, and we write $dim(V)$. If $U ⊆ V$ is a subspace of $V$, then $dim(U) ⩽ dim(V) $and $dim(U) =dim(V)$ if and only if $U = V$​. Intuitively, <strong>the dimension of a vector space</strong><br><strong>can be thought of as the number of independent directions in this vector space.</strong></p>
</li>
<li><p>The dimension of a vector space is not necessarily the number of elements in a vector.</p>
</li>
<li><p>A basis of a subspace $U = span[x_1, . . . , x_m] ⊆ \mathbb R^n$ can be found by executing the following steps</p>
<ol>
<li>Write the spanning vectors as columns of a matrix $A$.</li>
<li>Determine the row-echelon form of $A$.</li>
<li>The spanning vectors associated with the pivot columns are a basis of $U$.</li>
</ol>
<blockquote>
<p>e.g.  For a vector subspace $U ⊆ \mathbb R^5$, spanned by the vectors</p>
<script type="math/tex; mode=display">
x_1 = \left[ \begin{matrix}
1 \\
2 \\
-1 \\
-1 \\
-1
\end{matrix} \right],
x_2 = \left[ \begin{matrix}
2 \\
-1 \\
1 \\
2 \\
-2
\end{matrix} \right],
x_3 = \left[ \begin{matrix}
3 \\
-4 \\
3 \\
5 \\
-3
\end{matrix} \right],
x_4 = \left[ \begin{matrix}
-1 \\
8 \\
-5 \\
-6 \\
1
\end{matrix} \right] \in \mathbb R^5</script><p>To find out which vectors $x_1, . . . , x_4$ are a basis for $U$, we need to check whether $x_1, . . . , x_4$ are linearly independent.</p>
<p>Therefore, we need to solve $\sum_{i=1}^4{\lambda_ix_i}=0$,  which leads to a homogeneous system of equations with matrix</p>
<script type="math/tex; mode=display">
[x_1,x_2,x_3,x_4]=\left[\begin{matrix} 
1& 2& 3& -1\\
2& -1& -4& 8\\
-1& 1& 3& -5\\
-1& 2& 5& -6\\
-1& -2& -3& 1\\
\end{matrix}\right]</script><p>With the basic transformation rules for systems of linear equations, we obtain the row-echelon form</p>
<script type="math/tex; mode=display">
\left[\begin{matrix} 
1& 2& 3& -1\\
0& 1& 2& -2\\
0& 0& 0& 1\\
0& 0& 0& 0\\
0& 0& 0& 0\\
\end{matrix}\right]</script><p>We see from the row-echelon form that $x_1, x_2, x_4$ are linearly independent.Therefore, $\{x_1, x_2, x_4\}$ is abasis of $U$.</p>
</blockquote>
</li>
<li><p><strong>Rank</strong>: The <strong>number of linearly independent columns</strong> of a matrix $A ∈ \mathbb R^{m×n}$ equals the number of linearly independent rows and is called the <strong>rank</strong> of $A$ and is denoted by $rk(A)$.</p>
<ul>
<li>$rk(A) = rk(A^⊤)$, i.e., the column rank equals the row rank.</li>
<li>The columns of $A ∈ \mathbb R^{m×n}$ span a subspace $U ⊆ \mathbb R^m$ with $dim(U) =rk(A)$.</li>
<li>The rows of $A ∈ \mathbb R^{m×n}$ span a subspace $W ⊆ \mathbb R^n$ with $dim(W) =rk(A)$.</li>
<li>For all $A ∈ \mathbb R^{n×n}$ it holds that $A$ is regular (invertible) if and only if $rk(A) = n$.</li>
<li>For all $A ∈ \mathbb R^{m×n}$ and all $b ∈ \mathbb R^m$ it holds that the linear equation system $Ax = b$ can be solved if and only if $rk(A) = rk(A|b)$.</li>
<li>For$A ∈ \mathbb R^{m×n}$ the subspace of solutions for $Ax = 0$ possesses dimension $n − rk(A)$.</li>
<li>A matrix $A ∈ \mathbb R^{m×n}$  has <em>full rank</em> if its rank equals the largest possible  rank for a matrix of the same dimensions. This means that the rank of a full-rank matrix is the lesser of the number of rows and columns, i.e., $rk(A) = min(m,n)$. A matrix is said to be <em>rank deficient</em> if it does not  have full rank.</li>
</ul>
</li>
</ul>
<h2 id="Linear-Mappings"><a href="#Linear-Mappings" class="headerlink" title="Linear Mappings"></a>Linear Mappings</h2><ul>
<li><p><strong>Linear Mapping</strong>: For vector spaces $V,W$, a mapping $Φ : V → W$ is called a linear mapping (or vector space homomorphism/linear transformation) if</p>
<script type="math/tex; mode=display">
\forall x,y \in V \; \forall \lambda ,\psi \in \mathbb R:\varPhi(\lambda x + \psi y) =\lambda \varPhi(x) + \psi \varPhi(y)</script><p>(added together and multiplied by a scalar) It turns out that we can represent linear mappings as matrices.</p>
</li>
<li><p>Consider a mapping $Φ : V →W$, where $V,W$ can be arbitrary sets. Then $Φ$ is called</p>
<ul>
<li><strong><code>Injective</code></strong> if $∀x, y ∈ V : Φ(x) = Φ(y) ⇒ x = y$.</li>
<li><strong><code>Surjective</code></strong> if $Φ(V) = W$.   Every element in $W$ can be “reached” from $V$ using $Φ$.</li>
<li><strong><code>Bijective</code></strong> if it is injective and surjective.  A bijective $Φ$ can be “undone”, i.e., there exists a mapping $Ψ :W → V$ so that $Ψ ◦ Φ(x) = x$. This mapping $Ψ$ is then called the inverse of $Φ$ and normally denoted by $Φ^{−1}$.</li>
</ul>
</li>
<li><p>The special cases of linearmappings between vector spaces $V$ and $W$</p>
<ul>
<li><strong><code>Isomorphism</code></strong>: $Φ : V →W$ <strong>linear and bijective</strong></li>
<li><strong><code>Endomorphism</code></strong>: $Φ : V → V$ <strong>linear</strong></li>
<li><strong><code>Automorphism</code></strong>: $Φ : V → V$ <strong>linear and bijective</strong></li>
<li>We define $id_V : V → V, x \mapsto x$ as the identity mapping or identity automorphism in V.</li>
</ul>
</li>
</ul>
<blockquote>
<p>The mapping $Φ : \mathbb R^2 → \mathbb C, Φ(x) = x_1 + ix_2$, is a <strong>homomorphism</strong>:</p>
<script type="math/tex; mode=display">
\begin{equation}
\begin{split}
\varPhi \left( 
\left[\begin{matrix} 
x_1\\
x_2
\end{matrix}\right] +
\left[\begin{matrix} 
y_1\\
y_2
\end{matrix}\right] 
\right) &= (x_1+y_1) + i(x_2+y_2)=x_1+ix_2+y_1+iy_2 \\
&= \varPhi \left( 
\left[\begin{matrix} 
x_1\\
x_2
\end{matrix}\right]\right)+\varPhi \left( 
\left[\begin{matrix} 
y_1\\
y_2
\end{matrix}\right]\right)
\end{split}
\end{equation}</script><script type="math/tex; mode=display">
\left( \lambda \left[\begin{matrix} 
x_1\\
x_2
\end{matrix}\right]\right) = 
\lambda x_1 + \lambda i x_2 = 
\lambda(x_1+ix_2) =
\lambda \varPhi \left( \left[\begin{matrix} 
x_1\\
x_2
\end{matrix}\right]\right)</script></blockquote>
<ul>
<li><p><em>Finite-dimensional vector spaces $V$ and $W$</em> are <strong>isomorphic</strong> if and only if $dim(V) = dim(W)$.</p>
<ul>
<li>There exists a linear, bijective mapping between two vector spaces of the same dimension.</li>
<li>Vector spaces of the same dimension are kind of the same thing, as they can be transformed into each other without incurring any loss.</li>
<li>Treat $\mathbb R^{m×n}$ (the vector space of $m× n$-matrices) and $R^{mn}$ (the vector space of vectors of length $mn$) the same, as their dimensions are $mn$, and there exists a linear, bijective mapping that transforms one into the other.</li>
</ul>
</li>
<li><p>Consider vector spaces $V,W,X$</p>
<ul>
<li>For linear mappings $Φ : V → W$ and $Ψ : W → X$, the mapping $Ψ ◦ Φ : V → X$ is also linear.</li>
<li>If $Φ : V → W$ is an isomorphism, then $Φ^{−1}: W → V$ is an isomorphism, too.</li>
<li>If $Φ : V →W, Ψ : V →W$ are linear, then $Φ+Ψ$ and $λΦ, λ ∈ R$, are linear, too.</li>
</ul>
</li>
<li><p>Consider a basis $\{b_1, . . . , b_n\}$ of an $n$-dimensional vector space $V$. We write </p>
<script type="math/tex; mode=display">
B = (b_1,\cdots,b_n)</script><p>and call this $n$-tuple an <strong>ordered basis</strong> of $V$.</p>
</li>
<li><p><strong>Coordinates</strong>:  Consider a vector space $V$ and an ordered basis $B = (b_1, . . . , b_n)$ of $V$. For any $x ∈ V$ we obtain a unique representation (linear combination)</p>
<script type="math/tex; mode=display">
x = \alpha_1b_1 + \cdots + \alpha_nb_n</script><p>of $x$ with respect to $B$. Then $α_1, . . . , α_n$ are the <strong>coordinates</strong> of $x$ with respect to $B$, and the vector</p>
<script type="math/tex; mode=display">
\alpha = \left[
\begin{matrix}
\alpha_1\\
\vdots\\
\alpha_n
\end{matrix}
\right]\in \mathbb R^n</script><p>is the <strong>coordinate vector/coordinate representation</strong> of $x$ with respect to the ordered basis $B$​.  Different coordinate representations of a vector x, depending on the choice of basis.</p>
</li>
<li><p><strong>Transformation Matrix</strong>: Consider vector spaces $V,W$ with corresponding (ordered) bases $B = (b_1, . . . , b_n)$ and $C = (c_1, . . . , c_m)$. Moreover, we consider a linear mapping $Φ : V →W$. For $j ∈ \{1, . . . , n\}$,</p>
<script type="math/tex; mode=display">
\varPhi(b_j) = \alpha_{1j}c_1+\cdots+\alpha_{mj}c_m=\sum_{i=1}^m{\alpha_{ij}c_i}</script><p>is the unique representation of $Φ(b_j)$ with respect to $C$. Then, we call the $m× n$-matrix $A_Φ$, whose elements are given by $A_{\varPhi}(i,j)=\alpha_{ij}$ the <strong>transformation matrix</strong> of $Φ$ (with respect to the ordered bases $B$ of $V$ and $C$ of $W$).</p>
</li>
<li><p>The coordinates of $Φ(b_j)$ with respect to the ordered basis $C$ of $W$ are the $j$-th column of $A_Φ$. Consider (finite-dimensional) vector spaces $V,W$ with ordered bases $B, C$ and a linear mapping $Φ : V →W$ with  transformation matrix $A_Φ$. If $\tilde x$ is the coordinate vector of $x ∈ V$ with respect to $B$ and $\tilde y$ the coordinate vector of $y = Φ(x) ∈ W$ with respect<br>to $C$, then  </p>
<script type="math/tex; mode=display">
\tilde y = A_\varPhi \tilde x</script></li>
</ul>
<blockquote>
<p>e.g. Consider a homomorphism $Φ : V → W$ and ordered bases $B = (b_1, . . . , b_3)$ of $V$ and $C = (c_1, . . . , c_4)$ of $W$. With </p>
<script type="math/tex; mode=display">
\varPhi(b_1)=c_1-c_2+3c_3-c_4 \\
\varPhi(b_2)=2c_1+c_2+7c_3+2c_4\\
\varPhi(b_3)=3c_2+c_3+4c_4</script><p>the transformation matrix $A_Φ$ with respect to B and C satisfies $Φ(b_k) =\sum_{i=1}^4{α_{ik}c_i}$ for $k = 1, . . . , 3$ and is given as</p>
<script type="math/tex; mode=display">
A_\varPhi = [\alpha_1,\alpha_2,\alpha_3]
=\left[\begin{matrix}
1& 2& 0\\
-1& 1& 3\\
3& 7& 1\\
-1& 2& 4\\
\end{matrix}\right]</script><p>Linear Transformations of Vectors</p>
<p><img src="https://cdn.jsdelivr.net/gh/Eliauk-L/blog-img@img/img/202407251639897.png" alt="Linear Transformations of Vectors"></p>
<p>Three examples of linear transformations of the vectors shown as dots in (a); (b) Rotation by 45◦; (c) Stretching of the horizontal coordinates by 2; (d) Combination of reflection, rotation and stretching.</p>
<p>We consider three linear transformations of a set of vectors in $\mathbb R^2$ with the transformation matrices</p>
<script type="math/tex; mode=display">
A_1 = \left[\begin{matrix}
cos(\frac{\pi}{4})& -sin(\frac{\pi}{4})\\
sin(\frac{\pi}{4})& cos(\frac{\pi}{4})
\end{matrix}\right],
A_2 = \left[\begin{matrix}
2& 0\\
0& 1
\end{matrix}\right],
A_3 = \frac{1}{2}\left[\begin{matrix}
3& -1\\
1& -1
\end{matrix}\right],</script></blockquote>
<ul>
<li><p><strong>Basis Change</strong>: For a linear mapping $Φ : V →W$, ordered bases $B=(b_1,…,b_n),\tilde B=(\tilde{b_1},…,\tilde{b_n})$ of $V$ and $C=(c_1,…,c_m),\tilde C=(\tilde{c_1},…,\tilde{c_m})$ of $W$, and a transformation matrix $A_\varPhi$ of $\varPhi$  with respect to $B$ and $C$, the corresponding transformation matrix $\tilde A_Φ$ with respect to the bases $\tilde B$ and $\tilde C$ is given as</p>
<script type="math/tex; mode=display">
\tilde A_Φ = T^{-1}A_\varPhi S</script><p>$S ∈ \mathbb R^{n×n}$ is the transformation matrix of $id_V$ that maps coordinates with respect to<br>$\tilde B$ onto coordinates with respect to $B$, and $T ∈ \mathbb R^{m×m}$ is the transformation matrix of $id_W$that maps coordinates with respect to $\tilde C$ onto coordinates with respect to $C$.</p>
<ul>
<li><p>we can write the vectors of the new basis $\tilde B$ of $V$as a linear combination of the basis vectors of $B$, such that</p>
<script type="math/tex; mode=display">
\tilde b_j=s_{1j}b_1+\cdots+s_{nj}b_n=\sum_{i=1}^n{s_{ij}b_i},j = 1,...,n</script><script type="math/tex; mode=display">
\tilde c_k=t_{1k}c_1+\cdots+t_{mk}c_m=\sum_{l=1}^m{t_{lk}c_l},k = 1,...,m</script></li>
<li><p>We define $S = ((s_{ij})) ∈ \mathbb R^{n×n}$ as the transformation matrix that maps coordinates with respect to $\tilde B$ onto coordinates with respect to $B$ and $T = ((t_{lk})) ∈ R^{m×m}$ as the transformation matrix that maps coordinates with respect to $\tilde C$ onto coordinates with respect to $C$. </p>
</li>
<li><p>In particular, the $j$th column of $S$ is the coordinate representation of $\tilde b_j$ with respect to $B$ and the $k$th column of $T$ is the coordinate representation of $\tilde c_k$ with respect to $C$. Both $S$ and $T$ are regular.</p>
</li>
</ul>
</li>
<li><p><strong>Equivalence</strong>: Two matrices $A, \tilde A ∈ R^{m×n}$ are <strong>equivalent</strong> if there exist regular matrices $S ∈ \mathbb R^{n×n}$ and $T ∈ \mathbb R^{m×m}$, such that $\tilde A = T^{-1}AS$.</p>
</li>
<li><p><strong>Similarity</strong>: Two matrices $A, \tilde A ∈ \mathbb R^{n×n}$ are <strong>similar</strong> if there exists a regular matrix $S ∈ \mathbb R^{n×n}$ with $\tilde A = S^{−1}AS$.</p>
</li>
<li><p>Similar matrices are always equivalent. However, equivalent matrices are not necessarily similar.</p>
</li>
<li><p>Consider vector spaces $V,W,X$. We already know that for linear mappings $Φ : V → W$ and $Ψ : W → X$ the mapping $Ψ ◦ Φ : V → X$ is also linear. With transformation matrices $A_Φ$ and $A_Ψ$ of the corresponding mappings, the overall transformation matrix is $A_{Ψ◦Φ} = A_ΨA_Φ$.</p>
</li>
<li><p><strong>Image and Kernel</strong>： For $Φ : V →W$, we define the <strong>kernel/null space</strong></p>
<script type="math/tex; mode=display">
ker(\varPhi) := \varPhi^{-1}(0_W)=\{v\in V:\varPhi(v)=0_W\}</script><p>and the <strong>image/range</strong></p>
<script type="math/tex; mode=display">
Im(\varPhi):=\varPhi(V)=\{w\in W|\exists v\in V:\varPhi(v)=w \}</script><p>We also call $V$ and $W$ also the <strong>domain</strong> and <strong>codomain</strong> of $Φ$</p>
</li>
<li><p>The kernel is the set of vectors $v ∈ V$ that $Φ$ maps onto the neutral element $0_W ∈ W$. The image is the set of vectors $w ∈ W$ that can be “reached” by $Φ$ from any vector in $V$.<br><img src="https://cdn.jsdelivr.net/gh/Eliauk-L/blog-img@img/img/202407251639125.png" alt="Kernel and Image"></p>
</li>
<li><p>Consider a linear mapping $Φ : V → W$, where $V,W$ are vector spaces.</p>
<ul>
<li>It always holds that $Φ(0_V) = 0_W$ and, therefore, $0_V ∈ ker(Φ)$. In particular, the null space is never empty.</li>
<li>$Im(Φ) ⊆ W$ is a subspace of $W$, and $ker(Φ) ⊆ V$ is a subspace of $V$.</li>
<li>$Φ$ is injective (one-to-one) if and only if $ker(Φ) = {0}$.</li>
</ul>
</li>
<li><p><strong>Null Space and Column Space</strong>: Let us consider $A ∈ \mathbb R^{m×n}$ and a linear mapping $Φ : \mathbb R^n → \mathbb R^m, x \mapsto Ax$.</p>
<ul>
<li><p>For $A = [a_1, . . . , a_n]$, where $a_i$ are the columns of $A$, we obtain</p>
<script type="math/tex; mode=display">
\begin{equation}
\begin{split}
Im(\varPhi) &=\{Ax:x\in \mathbb R^n \}=\{\sum_{i=1}^{n}{x_ia_i}:x_1, . . . , x_n \in \mathbb R\} \\
&=span[a_1,...,a_n]\subseteq \mathbb R^m
\end{split}
\end{equation}</script><p>the image is the span of the columns of $A$, also called the <strong>column space</strong>. Therefore, the column space (image) is a subspace of $\mathbb R^m$, where $m$ is the “height” of the matrix.</p>
</li>
<li><p>$rk(A) = dim(Im(Φ))$.</p>
</li>
<li><p>The kernel/null space $ker(Φ)$ is the general solution to the homogeneous system of linear equations $Ax = 0$ and captures all possible linear combinations of the elements in $\mathbb R^n$ that produce $0 ∈ \mathbb R^m$.</p>
</li>
<li><p>The kernel is a subspace of $\mathbb R^n$, where $n$ is the “width” of the matrix.</p>
</li>
</ul>
<blockquote>
<p>e.g.  The mapping</p>
<script type="math/tex; mode=display">
\varPhi:\mathbb R^4 \rightarrow \mathbb R^2, \left[\begin{matrix} 
x_1\\
x_2\\
x_3\\
x_4
\end{matrix} \right] \mapsto
\left[\begin{matrix} 
x_1\\
x_2\\
x_3\\
x_4
\end{matrix} \right]
\left[\begin{matrix} 
1& 2& -1& 0\\
1& 0& 0& 1
\end{matrix} \right] = \left[\begin{matrix} 
x_1+2x_2-x_3\\
x_1+x_4
\end{matrix} \right]\\
=x_1\left[\begin{matrix} 
1\\
1
\end{matrix} \right] + x_2\left[\begin{matrix} 
2\\
0
\end{matrix} \right] +x_3\left[\begin{matrix} 
-1\\
0
\end{matrix} \right] +x_4\left[\begin{matrix} 
0\\
1
\end{matrix} \right]</script><p>is linear. To determine $Im(Φ)$, we can take the span of the columns of the transformation matrix and obtain</p>
<script type="math/tex; mode=display">
Im(\varPhi)=span
\left[
\left[\begin{matrix} 
1\\
1
\end{matrix} \right],
\left[\begin{matrix} 
2\\
0
\end{matrix} \right],
\left[\begin{matrix} 
-1\\
0
\end{matrix} \right],
\left[\begin{matrix} 
0\\
1
\end{matrix} \right]
\right]</script><p>To compute the kernel (null space) of $Φ$, we need to solve $Ax = 0$, </p>
<script type="math/tex; mode=display">
\left[\begin{matrix}
1& 2& -1& 0\\
1& 0& 0& 1
\end{matrix}\right] \rightarrow \cdots \rightarrow
\left[\begin{matrix}
1& 0& 0& 1\\
0& 1& -\frac{1}{2}& -\frac{1}{2}
\end{matrix}\right]</script><p>Therefore, $0 = a_3+ \frac{1}{2}a_2$, $0 = a_1−\frac{1}{2}a_2−a_4$</p>
<script type="math/tex; mode=display">
ker(\varPhi) = span 
\left[
\left[\begin{matrix}
0\\
\frac{1}{2}\\
1\\
0
\end{matrix}\right],
\left[\begin{matrix}
-1\\
\frac{1}{2}\\
0\\
1
\end{matrix}\right]
\right]</script></blockquote>
</li>
<li><p><strong>Rank-Nullity Theorem</strong>: For vector spaces $V,W$ and a linear mapping $Φ : V →W$ it holds that </p>
<script type="math/tex; mode=display">
dim(ker(Φ)) + dim(Im(Φ)) = dim(V)</script><ul>
<li><p>If $dim(Im(Φ)) &lt; dim(V)$, then $ker(Φ)$ is non-trivial, i.e., the kernel contains more than $0_V$ and $dim(ker(Φ)) ⩾ 1$.</p>
</li>
<li><p>If $A_Φ$ is the transformation matrix of $Φ$ with respect to an ordered basis and $dim(Im(Φ)) &lt; dim(V)$, then the system of linear equations $A_Φx = 0$ has infinitely many solutions.</p>
</li>
<li><p>If $dim(V) = dim(W)$, then the following three-way equivalence holds:</p>
<ul>
<li>$Φ$ is injective</li>
<li>$Φ$ is surjective</li>
<li>$Φ$ is bijective</li>
</ul>
<p>since $Im(Φ) ⊆ W$.</p>
</li>
</ul>
</li>
</ul>
<blockquote>
<p><strong>Bibliography:</strong></p>
<ol>
<li>Mathematics for Machine Learning_Marc Peter Deisenroth_2020</li>
</ol>
</blockquote>

                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        文章作者:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/about" rel="external nofollow noreferrer">Jay</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        文章链接:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="http://Eliauk-L.github.io/2024/07/25/linearalgebra/">http://Eliauk-L.github.io/2024/07/25/linearalgebra/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        版权声明:
                    </i>
                </span>
                <span class="reprint-info">
                    本博客所有文章除特別声明外，均采用
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    许可协议。转载请注明来源
                    <a href="/about" target="_blank">Jay</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>复制成功，请遵循本文的转载规则</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">查看</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/tags/Mathematics/">
                                    <span class="chip bg-color">Mathematics</span>
                                </a>
                            
                                <a href="/tags/%E7%AC%94%E8%AE%B0/">
                                    <span class="chip bg-color">笔记</span>
                                </a>
                            
                                <a href="/tags/LinearAlgebra/">
                                    <span class="chip bg-color">LinearAlgebra</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="qq, qzone, wechat, weibo, douban" data-wechat-qrcode-helper="<p>微信扫一扫即可分享！</p>"></div>
    <script src="/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;上一篇</div>
            <div class="card">
                <a href="/2024/07/26/scrapy-pa-chong-xi-lie-zhi-scrapy-jian-jie/">
                    <div class="card-image">
                        
                        
                        <img src="/medias/featureimages/34.jpg" class="responsive-img" alt="Scrapy爬虫系列之Scrapy简介">
                        
                        <span class="card-title">Scrapy爬虫系列之Scrapy简介</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2024-07-26
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/categories/%E7%88%AC%E8%99%AB/" class="post-category">
                                    爬虫
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/tags/%E7%AC%94%E8%AE%B0/">
                        <span class="chip bg-color">笔记</span>
                    </a>
                    
                    <a href="/tags/Python/">
                        <span class="chip bg-color">Python</span>
                    </a>
                    
                    <a href="/tags/%E7%88%AC%E8%99%AB/">
                        <span class="chip bg-color">爬虫</span>
                    </a>
                    
                    <a href="/tags/Scrapy/">
                        <span class="chip bg-color">Scrapy</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                下一篇&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/2024/07/24/scrapy-pa-chong-xi-lie-zhi-selenium-cao-zong-liu-lan-qi/">
                    <div class="card-image">
                        
                        
                        <img src="/medias/featureimages/20.jpg" class="responsive-img" alt="Scrapy爬虫系列之Selenium操纵浏览器">
                        
                        <span class="card-title">Scrapy爬虫系列之Selenium操纵浏览器</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2024-07-24
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/categories/%E7%88%AC%E8%99%AB/" class="post-category">
                                    爬虫
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/tags/%E7%AC%94%E8%AE%B0/">
                        <span class="chip bg-color">笔记</span>
                    </a>
                    
                    <a href="/tags/Python/">
                        <span class="chip bg-color">Python</span>
                    </a>
                    
                    <a href="/tags/%E7%88%AC%E8%99%AB/">
                        <span class="chip bg-color">爬虫</span>
                    </a>
                    
                    <a href="/tags/Selenium/">
                        <span class="chip bg-color">Selenium</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- 代码块功能依赖 -->
<script type="text/javascript" src="/libs/codeBlock/codeBlockFuction.js"></script>


  <!-- 是否加载使用自带的 prismjs. -->
  <script type="text/javascript" src="/libs/prism/prism.min.js"></script>


<!-- 代码语言 -->

<script type="text/javascript" src="/libs/codeBlock/codeLang.js"></script>


<!-- 代码块复制 -->

<script type="text/javascript" src="/libs/codeBlock/codeCopy.js"></script>


<!-- 代码块收缩 -->

<script type="text/javascript" src="/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;目录</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC 悬浮按钮. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* 修复文章卡片 div 的宽度. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // 切换TOC目录展开收缩的相关操作.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>

    

</main>


<script src="https://cdn.bootcss.com/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
<script>
    MathJax.Hub.Config({
        tex2jax: {inlineMath: [['$', '$'], ['\\(', '\\)']]}
    });
</script>



    <footer class="page-footer bg-color">
    

    <div class="container row center-align"
         style="margin-bottom: 0px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024</span>
            
            <a href="/about" target="_blank">Jay</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;总访问量:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;总访问人数:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- 运行天数提醒. -->
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Eliauk-L" class="tooltipped" target="_blank" data-tooltip="访问我的GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:2571368706@qq.com" class="tooltipped" target="_blank" data-tooltip="邮件联系我" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>







    <a href="tencent://AddContact/?fromId=50&fromSubId=1&subcmd=all&uin=2571368706" class="tooltipped" target="_blank" data-tooltip="QQ联系我: 2571368706" data-position="top" data-delay="50">
        <i class="fab fa-qq"></i>
    </a>







</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- 搜索遮罩框 -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;搜索</span>
            <input type="search" id="searchInput" name="s" placeholder="请输入搜索的关键字"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- 白天和黑夜主题 -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- 回到顶部按钮 -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/libs/materialize/materialize.min.js"></script>
    <script src="/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/libs/aos/aos.js"></script>
    <script src="/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/js/matery.js"></script>

    

    
    
    

    <!-- 雪花特效 -->
     
        <script type="text/javascript">
            // 只在桌面版网页启用特效
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/libs/others/snow.js"><\/script>');
            }
        </script>
    

    <!-- 鼠标星星特效 -->
    

    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--腾讯兔小巢-->
    
    

    

    

    
    <script src="/libs/instantpage/instantpage.js" type="module"></script>
    

</body>

</html>
