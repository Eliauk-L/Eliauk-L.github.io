<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="Dimensionality Reduction with Principal Component Analysis, Jay&#39;s Blog">
    <meta name="description" content="Dimensionality Reduction with Principal Component Analysis
Dimensionality reduction exploits structure and correlation a">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>Dimensionality Reduction with Principal Component Analysis | Jay&#39;s Blog</title>
    <link rel="icon" type="image/png" href="/favicon.png">
    


    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/css/matery.css">
<link rel="stylesheet" type="text/css" href="/css/my.css">
<link rel="stylesheet" type="text/css" href="/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/css/post.css">




    



    <script src="/libs/jquery/jquery-3.6.0.min.js"></script>

    <script src="https://sdk.jinrishici.com/v2/browser/jinrishici.js" charset="utf-8"></script>

<meta name="generator" content="Hexo 7.2.0">
<style>.github-emoji { position: relative; display: inline-block; width: 1.2em; min-height: 1.2em; overflow: hidden; vertical-align: top; color: transparent; }  .github-emoji > span { position: relative; z-index: 10; }  .github-emoji img, .github-emoji .fancybox { margin: 0 !important; padding: 0 !important; border: none !important; outline: none !important; text-decoration: none !important; user-select: none !important; cursor: auto !important; }  .github-emoji img { height: 1.2em !important; width: 1.2em !important; position: absolute !important; left: 50% !important; top: 50% !important; transform: translate(-50%, -50%) !important; user-select: none !important; cursor: auto !important; } .github-emoji-fallback { color: inherit; } .github-emoji-fallback img { opacity: 0 !important; }</style>
</head>


<body>
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/" class="waves-effect waves-light">
                    
                    <span class="logo-span">Jay&#39;s Blog</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>首页</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>标签</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>分类</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>归档</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/about" class="waves-effect waves-light">
      
      <i class="fas fa-user-circle" style="zoom: 0.6;"></i>
      
      <span>关于</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/galleries" class="waves-effect waves-light">
      
      <i class="fas fa-image" style="zoom: 0.6;"></i>
      
      <span>相册</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="搜索" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="深色/浅色模式" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <div class="logo-name">Jay&#39;s Blog</div>
        <div class="logo-desc">
            
            Jay&#39;s Blog
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			首页
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			标签
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			分类
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			归档
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/about" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-user-circle"></i>
			
			关于
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/galleries" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-image"></i>
			
			相册
		</a>
          
        </li>
        
        
    </ul>
</div>


        </div>

        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('/medias/featureimages/19.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">Dimensionality Reduction with Principal Component Analysis</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- 文章内容详情 -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/tags/Mathematics/">
                                <span class="chip bg-color">Mathematics</span>
                            </a>
                        
                            <a href="/tags/%E7%AC%94%E8%AE%B0/">
                                <span class="chip bg-color">笔记</span>
                            </a>
                        
                            <a href="/tags/MachineLearing/">
                                <span class="chip bg-color">MachineLearing</span>
                            </a>
                        
                            <a href="/tags/DimensionalityReduction/">
                                <span class="chip bg-color">DimensionalityReduction</span>
                            </a>
                        
                            <a href="/tags/PCA/">
                                <span class="chip bg-color">PCA</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/categories/MachineLearing/" class="post-category">
                                MachineLearing
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>发布日期:&nbsp;&nbsp;
                    2024-08-11
                </div>
                

                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>文章字数:&nbsp;&nbsp;
                    1.7k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>阅读时长:&nbsp;&nbsp;
                    10 分
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>阅读次数:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        
        <!-- 是否加载使用自带的 prismjs. -->
        <link rel="stylesheet" href="/libs/prism/prism.min.css">
        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <h1 id="Dimensionality-Reduction-with-Principal-Component-Analysis"><a href="#Dimensionality-Reduction-with-Principal-Component-Analysis" class="headerlink" title="Dimensionality Reduction with Principal Component Analysis"></a>Dimensionality Reduction with Principal Component Analysis</h1><ul>
<li>Dimensionality reduction exploits structure and correlation and allows us to work with a more compact representation of the data, ideally without losing information.</li>
<li><strong><em>principal component analysis (PCA)</em></strong>, an algorithm for linear dimensionality reduction.</li>
</ul>
<h2 id="Problem-Setting"><a href="#Problem-Setting" class="headerlink" title="Problem Setting"></a>Problem Setting</h2><ul>
<li><p>In PCA, we are interested in finding projections $\tilde x_n$ of data points xn that are as similar to the original data points as possible, but which have a significantly lower intrinsic dimensionality.</p>
</li>
<li><p>More concretely, we consider an i.i.d. dataset $X = \{x_1, . . . , x_N\}, x_n ∈\mathbb R^D$, with mean $0$ that possesses the <strong><em>data covariance matrix</em></strong></p>
<script type="math/tex; mode=display">
S=\frac{1}{N}\sum_{n=1}^N{x_nx_n^T}</script><p>Furthermore, we assume there exists a low-dimensional compressed representation (code)</p>
<script type="math/tex; mode=display">
z_n=B^Tx_n\in\mathbb R^M</script><p>of $x_n$, where we define the projection matrix</p>
<script type="math/tex; mode=display">
B:=\left[\begin{matrix}b_1,...,b_M\end{matrix}\right]\in\mathbb R^{D\times M}</script><p>We assume that the columns ofB are orthonormal so that $b^⊤_ib_j = 0$ if and only if $i \ne j$ and $b^⊤_i b_i = 1$. We seek an $M$-dimensiona lsubspace $U ⊆\mathbb R^D$, $dim(U) = M &lt; D$ onto which we project the data. We denote the projected data by $\tilde x_n ∈ U$, and their coordinates (with respect to the basis vectors $b_1, . . . , b_M$ of $U$) by $z_n$. Our aim is to find projections $\tilde x_n ∈\mathbb R^D$ (or equivalently the codes $z_n$ and the basis vectors $b_1, . . . , b_M$) so that they are as similar to the original data $x_n$ and minimize the loss due to compression.</p>
</li>
<li><p>The columns $b_1, . . . , b_M$ of $B$ form a basis of the $M$-dimensional subspace in which<br>the projected data $\tilde x= BB^⊤x ∈\mathbb R^D$ live.</p>
<blockquote>
<p>e.g <strong>Coordinate Representation/Code</strong></p>
<p>Consider $\mathbb R^2$ with the canonical basis $e_1 = [1, 0]^⊤, e_2 = [0, 1]^⊤$. When we consider vectors of the form </p>
<script type="math/tex; mode=display">
\tilde x_n = \left[\begin{matrix}0\\z\end{matrix}\right]\in\mathbb R^2,z\in\mathbb R</script><p>they can always be written as $0e_1 + ze_2$. To represent these vectors it is sufficient to remember/store the <strong><em>coordinate/code</em></strong> $z$ of $\tilde x$ with respect to the $e_2$ vector.</p>
</blockquote>
</li>
<li><p>In PCA, we consider a linear relationship between the original data $x$ and its low-dimensional code $z$ so that $z = B^⊤x$ and $\tilde x= Bz$ for a suitable matrix $B$.</p>
</li>
</ul>
<h2 id="Maximum-Variance-Perspective"><a href="#Maximum-Variance-Perspective" class="headerlink" title="Maximum Variance Perspective"></a>Maximum Variance Perspective</h2><ul>
<li><p>We know that the variance is an indicator of the spread of the data, and we can derive PCA as a dimensionality reduction algorithm that maximizes the variance in the low-dimensional representation of the data to retain as much information as possible.</p>
</li>
<li><p>Considering the setting, our aim is to find a matrix $B$ that retains as much information as possible when compressing data by projecting it onto the subspace spanned by the columns $b_1, . . . , b_M$ of $B$. Retaining most information after data compression is equivalent to capturing the largest amount of variance in the low-dimensional code.</p>
</li>
<li><p>For the data covariance matrix , we assumed centered data. We can make this assumption without loss of generality: Let us assume that $\mu$ is the mean of the data. Using the properties of the variance, we obtain</p>
<script type="math/tex; mode=display">
\mathbb V_z[z]=\mathbb V_x[B^T(x-\mu)]=\mathbb V_x[B^Tx-B^T\mu]=\mathbb V_x[B^Tx]</script><p>i.e., <em>the variance of the low-dimensional code does not depend on the mean of the data.</em> Therefore, we assume without loss of generality that the data has mean $0$ for the remainder of this section. With this assumption the mean of the low-dimensional code is also $0$ since $\mathbb E_z[z] =\mathbb E_x[B^⊤x] =B^⊤\mathbb E_x[x] = 0$.</p>
</li>
</ul>
<h3 id="Direction-with-Maximal-Variance"><a href="#Direction-with-Maximal-Variance" class="headerlink" title="Direction with Maximal Variance"></a>Direction with Maximal Variance</h3><ul>
<li><p>We start by seeking a single vector $b_1 ∈\mathbb R^D$ that maximizes the variance of the projected data, i.e., we aim to maximize the variance of the first coordinate $z_1$ of $z ∈\mathbb R^M$ so that</p>
<script type="math/tex; mode=display">
V_1:=\mathbb V[z_1]=\frac{1}{N}\sum_{n=1}^Nz_{1n}^2</script><p>is maximized, where we exploited the i.i.d. assumption of the data and defined $z_{1n}$ as the first coordinate of the low-dimensional representation $z_n ∈\mathbb R^M$ of $x_n ∈\mathbb R^D$. Note that first component of $z_n$ is given by</p>
<script type="math/tex; mode=display">
z_{1n}=b_1^Tx_n</script><p>i.e., it is the coordinate of the orthogonal projection of $x_n$ onto the one-dimensional subspace spanned by $b_1$. We substitute $z_{1n}$, which yields</p>
<script type="math/tex; mode=display">
\begin{split}
V_1&=\frac{1}{N}\sum_{n=1}^N(b_1^Tx_n)^2=\frac{1}{N}\sum_{n=1}^Nb_1^Tx_nx_n^Tb_1\\
&=b^T_1\left(\frac{1}{N}\sum_{n=1}^Nx_nx_n^T\right)b_1=b_1^TSb_1
\end{split}</script><p>where $S$ is the data covariance matrix. We have used the fact that the dot product of two vectors is symmetric with respect to its arguments, that is, $b^⊤_1 x_n = x^⊤_n b_1$.</p>
</li>
<li><p>Notice that arbitrarily increasing the magnitude of the vector $b_1$ increases $V_1$, that is, a vector $b_1$ that is two times longer can result in $V_1$ that is potentially four times larger. Therefore, we restrict all solutions to $∥b1∥^2 = 1$, which results in a constrained optimization problem in which we seek the direction along which the data varies most.</p>
</li>
<li><p>With the restriction of the solution space to unit vectors the vector $b_1$ that points in the direction of maximum variance can be found by the constrained optimization problem</p>
<script type="math/tex; mode=display">
\underset{b_1}{\max}b_1^TSb_1\\
\text{subject to} \left\|b_1\right\|^2=1</script><p>we obtain the Lagrangian</p>
<script type="math/tex; mode=display">
\mathfrak L(b_1,\lambda)=b_1^TSb_1+\lambda_1(1-b_1^Tb_1)</script><p>to solve this constrained optimization problem. The partial derivatives of $\mathfrak L$ with respect to $b_1$ and $λ_1$ are</p>
<script type="math/tex; mode=display">
\frac{\partial \mathfrak L}{\partial b_1}=2b_1^TS-2\lambda_1b_1^T,\quad \frac{\partial \mathfrak L}{\partial \lambda_1}=1-b_1^Tb_1</script><p>respectively. Setting these partial derivatives to $0$ gives us the relations</p>
<script type="math/tex; mode=display">
Sb_1=\lambda_1b_1\\
b_1^Tb_1=1</script><p>By comparing this with the definition of an eigenvalue decomposition, we see that $b_1$ is an eigenvector of the data covariance matrix $S$, and the Lagrange multiplier $λ_1$ plays the role of the corresponding eigenvalue. This eigenvector property allows us to rewrite our variance objective as</p>
<script type="math/tex; mode=display">
V_1=b_1^TSb_1=\lambda_1b_1^Tb_1=\lambda_1</script><p>i.e., the variance of the data projected onto a one-dimensional subspace equals the eigenvalue that is associated with the basis vector $b_1$ that spans this subspace.</p>
</li>
<li><p>Therefore, to maximize the variance of the low-dimensional code, we choose the basis vector associated with the largest eigenvalue of the data covariance matrix. This eigenvector is called the <strong><em>first principal component</em></strong>. We can determine the effect/contribution of the principal component $b1$ in the original data space by mapping the coordinate $z_{1n}$ back into data space, which gives us the projected data point</p>
<script type="math/tex; mode=display">
\tilde x_n=b_1z_{1n}=b_1b_1^Tx_n\in\mathbb R^D</script><p>in the original data space.</p>
</li>
<li><p>Although $\tilde x_n$ is a $D$-dimensional vector, it only requires a single coordinate $z_{1n}$ to represent it with respect to the basis vector $b_1 ∈\mathbb R^D$</p>
</li>
</ul>
<h3 id="M-dimensional-Subspace-with-Maximal-Variance"><a href="#M-dimensional-Subspace-with-Maximal-Variance" class="headerlink" title="M-dimensional Subspace with Maximal Variance"></a>M-dimensional Subspace with Maximal Variance</h3><ul>
<li><p>Assume we have found the first $m− 1$ principal components as the $m− 1$ eigenvectors of $S$ that are associated with the largest $m− 1$ eigenvalues. Since$ S$ is symmetric, the spectral theorem states that we can use these eigenvectors to construct an orthonormal eigenbasis of an $(m − 1)$-dimensional subspace of $\mathbb R^D$.</p>
</li>
<li><p>Generally, the mth principal component can be found by subtracting the effect of the first $m− 1$ principal components $b_1, . . . , b_{m−1}$ from the data, thereby trying to find principal<br>components that compress the remaining information. We then arrive at the new data matrix</p>
<script type="math/tex; mode=display">
\hat X:=X-\sum_{i=1}^{m-1}b_ib_i^TX=X-B_{m-1}X</script><p>where $X = [x_1, . . . , x_N] ∈\mathbb R^{D×N}$ contains the data points as column vectors and $B_{m-1}:=\sum_{i=1}^{m-1}b_ib_i^T$  is a projection matrix that projects onto the subspace spanned by $b_1, . . . , b_{m−1}$.</p>
</li>
<li><p>To find the $m$th principal component, we maximize the variance</p>
<script type="math/tex; mode=display">
V_m=\mathbb V[z_m]=\frac{1}{N}\sum_{n=1}^Nz^2_{mn}=\frac{1}{N}\sum_{n=1}^N(b_m^T\hat x_n)^2=b^T_m\hat Sb_m</script><p>subject to $∥b_m∥^2 = 1$, where we followed the same steps and defined $\hat S$ as the data covariance matrix of the transformed dataset $\hat X := \{\hat x_1, . . . , \hat x_N\}$. As previously, when we looked at the first principal component alone, we solve a constrained optimization problem and discover that the optimal solution bm is the eigenvector of $\hat S$ that is associated with the largest eigenvalue of $\hat S$.</p>
</li>
<li><p>It turns out that bm is also an eigenvector of $S$. More generally, the sets of eigenvectors of $S$ and $\hat S$ are identical. Since both $S$ and $\hat S$ are symmetric, we can find an ONB of eigenvectors, i.e., there exist $D$ distinct eigenvectors for both $S$ and $\hat S$. Next, we show that<br>every eigenvector of $S$ is an eigenvector of $\hat S$. Assume we have already found eigenvectors $b_1, . . . , b_{m−1}$ of $\hat S$. Consider an eigenvector $b_i$ of $S$, i.e., $Sb_i = λ_ib_i$. In general,</p>
<script type="math/tex; mode=display">
\begin{split}
\hat Sb_i&=\frac{1}{N}\hat X\hat X^Tb_i=\frac{1}{N}(X-B_{m-1}X)(X-B_{m-1}X)^Tb_i\\
&=(S-SB_{m-1}-B_{m-1}S+B_{m-1}SB_{m-1})b_i
\end{split}</script><p>We distinguish between two cases. If $i ⩾ m$, i.e., $b_i$ is an eigenvector that is not among the first $m−1$ principal components, then bi is orthogonal to the first $m−1$ principal components and $B_{m−1}b_i = 0$. If $i &lt; m$, i.e., $b_i$ is among the first $m− 1$ principal components, then $b_i$ is a basis vector of the principal subspace onto which $B_{m−1}$ projects. Since $b_1, . . . , b_{m−1}$ are an ONB of this principal subspace, we obtain $B_{m−1}b_i = b_i$. The two cases can be summarized as follows:</p>
<script type="math/tex; mode=display">
B_{m-1}b_i=b_i\quad \text{if}\;i<m,\quad\quad B_{m-1}b_i=0\quad \text{if}\;i\ge m</script></li>
<li><p>In the case $i ⩾ m$, we obtain $\hat Sb_i = (S−B_{m−1}S)b_i = Sb_i = λ_ib_i $, i.e., $b_i$ is also an eigenvector of $\hat S$ with eigenvalue $λ_i$. Specifically,</p>
<script type="math/tex; mode=display">
\hat Sb_m=Sb_m=\lambda_mb_m</script><p>Equation reveals that $b_m$ is not only an eigenvector of $S$ but also of $\hat S$. Specifically, $λ_m$ is the largest eigenvalue of $\hat S$ and $λ_m$ is the $m$th largest eigenvalue of $S$, and both have the associated eigenvector $b_m$.</p>
</li>
<li><p>In the case $i &lt; m$​, we obtain </p>
<script type="math/tex; mode=display">
\hat Sb_i = (S-SB_{m-1}-B_{m-1}S+B_{m-1}SB_{m-1})b_i=0=0b_i</script><p>This means that $b_1, . . . , b_{m−1}$ are also eigenvectors of $\hat S$, but they are associated with eigenvalue $0$ so that $b_1, . . . , b_{m−1}$ span the null space of $\hat S$.</p>
</li>
<li><p>Overall, every eigenvector of $S$ is also an eigenvector of $\hat S$. However, if the eigenvectors of $S$ are part of the $(m − 1)$ dimensional principal subspace, then the associated eigenvalue of $\hat S$ is $0$.</p>
</li>
<li><p>With the relation $\hat Sb_m=Sb_m=\lambda_mb_m$ and $b^T_mb_m = 1$, the variance of the data projected onto the $m$th principal component is</p>
<script type="math/tex; mode=display">
V_m=b_m^TSb_m=\lambda_mb_m^Tb_m=\lambda_m</script><p>This means that the variance of the data, when projected onto an $M$-dimensional subspace, equals the sum of the eigenvalues that are associated with the corresponding eigenvectors of the data covariance matrix.</p>
</li>
<li><p>Overall, to find an $M$-dimensional subspace of $\mathbb R^D$ that retains as much information as possible, PCA tells us to choose the columns of the matrix $B$ as the M eigenvectors of the data covariance matrix $S$ that are associated with the $M$ largest eigenvalues. The maximum amount of variance PCA can capture with the first $M$ principal components is</p>
<script type="math/tex; mode=display">
V_M=\sum_{m=1}^M\lambda_m</script><p>where the $λ_m$ are the $M$ largest eigenvalues of the data covariance matrix $S$. Consequently, the variance lost by data compression via PCA is</p>
<script type="math/tex; mode=display">
J_M:=\sum_{j=M+1}^D\lambda_j=V_D-V_M</script><p>Instead of these absolute quantities, we can define the relative variance captured as $\frac{V_M}{V_D}$ , and the relative variance lost by compression as $1 − \frac{V_M}{V_D}$.</p>
</li>
</ul>
<h2 id="Projection-Perspective"><a href="#Projection-Perspective" class="headerlink" title="Projection Perspective"></a>Projection Perspective</h2><ul>
<li>We will look at the difference vectors between the original data $x_n$ and their reconstruction $\tilde x_n$ and minimize this distance so that $x_n$ and $\tilde x_n$ are as close as possible.</li>
</ul>
<h3 id="Setting-and-Objective"><a href="#Setting-and-Objective" class="headerlink" title="Setting and Objective"></a>Setting and Objective</h3><ul>
<li><p>Assume an (ordered) orthonormal basis (ONB) $B = (b_1, . . . , b_D)$ of $\mathbb R^D$, i.e., $b^⊤_i b_j = 1$ if and only if $i = j$ and $0$ otherwise.</p>
</li>
<li><p>We know that for a basis $(b_1, . . . , b_D)$ of $\mathbb R^D$ any $x ∈\mathbb R^D$ can be written as a linear combination of the basis vectors of $\mathbb R^D$, i.e.,</p>
<script type="math/tex; mode=display">
x=\sum_{d=1}^D\zeta_db_d=\sum_{m=1}^M\zeta_mb_m+\sum_{j=M+1}^D\zeta_jb_j</script><p>for suitable coordinates $\zeta_d ∈\mathbb R$.</p>
</li>
<li><p>We are interested in finding vectors $\tilde x∈ \mathbb R^D$, which live in lower-dimensional subspace $U ⊆ \mathbb R^D$, $dim(U) = M$, so that</p>
<script type="math/tex; mode=display">
\tilde x=\sum_{m=1}^Mz_mb_m\in U\subseteq \mathbb R^D</script><p>is as similar to $x$ as possible. Note that at this point we need to assume that the coordinates zm of $\tilde x$ and $\zeta_m$ of $x$ are not identical.</p>
</li>
<li><p>We use exactly this kind of representation of $\tilde x$ to find optimal coordinates $z$ and basis vectors $b_1, . . . , b_M$ such that $\tilde x$ is as similar to the original data point $x$ as possible, i.e., we aim to minimize the (Euclidean) distance $∥x − \tilde x∥$.</p>
</li>
<li><p>Without loss of generality, we assume that the dataset $X = \{x_1, . . . , x_N\},x_n ∈ \mathbb R^D$, is centered at $0$, i.e., $\mathbb E[X] = 0$. Without the zero-mean assumption, we would arrive at exactly the same solution, but the notation would be substantially more cluttered.</p>
</li>
<li><p>We are interested in finding the best linear projection of $X$ onto a lower-dimensional subspace $U$ of $\mathbb R^D$ with $dim(U) = M$ and orthonormal basis vectors $b_1, . . . , b_M$. We will call this subspace $U$ the <strong><em>principal subspace</em></strong>. The projections of the data points are denoted by</p>
<script type="math/tex; mode=display">
\tilde x_n:=\sum_{m=1}^Mz_{mn}b_m=Bz_n\in\mathbb R^D</script><p>where $z_n := [z_{1n}, . . . , z_{Mn}]^⊤ ∈\mathbb R^M$ is the coordinate vector of $\tilde x_n$ with respect to the basis $(b_1, . . . , b_M)$. More specifically, we are interested in having the $\tilde x_n$ as similar to $x_n$ as possible.</p>
</li>
<li><p>The similarity measure we use in the following is the squared distance (Euclidean norm) $∥x −\tilde x∥^2$ between $x$ and $\tilde x$. We therefore define our objective as minimizing the average squared Euclidean distance (<em>reconstruction error</em>)</p>
<script type="math/tex; mode=display">
J_M:=\frac{1}{N}\left\|x_n-\tilde x_n\right\|^2</script><p>where we make it explicit that the dimension of the subspace onto which we project the data is $M$. In order to find this optimal linear projection, we need to find the orthonormal basis of the principal subspace and the coordinates $z_n ∈\mathbb R^M$ of the projections with respect to this basis.</p>
</li>
<li><p>To find the coordinates $z_n$ and the ONB of the principal subspace, we follow a two-step approach. First, we optimize the coordinates $z_n$ for a given ONB $(b_1, . . . , b_M)$; second, we find the optimal ONB.</p>
</li>
</ul>
<h3 id="Finding-Optimal-Coordinates"><a href="#Finding-Optimal-Coordinates" class="headerlink" title="Finding Optimal Coordinates"></a>Finding Optimal Coordinates</h3><ul>
<li><p>Geometrically speaking, finding the optimal coordinates z corresponds to finding the representation of the linear projection $\tilde x$ with respect to b that minimizes the distance between $\tilde x−x$.</p>
</li>
<li><p>We assume an ONB $(b_1, . . . , b_M)$ of $U ⊆\mathbb R^D$. To find the optimal coordinates $z_m$ with respect to this basis, we require the partial derivatives</p>
<script type="math/tex; mode=display">
\begin{split}
\frac{\partial J_M}{\partial z_{in}}&=\frac{\partial J_M}{\partial \tilde x_n}\frac{\partial \tilde x_n}{\partial z_{in}}\\
\frac{\partial J_M}{\partial \tilde x_n}&=-\frac{2}{N}(x_n-\tilde x_n)^T\in\mathbb R^{1\times D}\\
\frac{\partial \tilde x_n}{\partial z_{in}}&=\frac{\partial}{\partial z_{in}}\left(\sum_{m=1}^Mz_{mn}b_m\right)=b_i
\end{split}</script><p>for $i = 1, . . . ,M$, such that we obtain</p>
<script type="math/tex; mode=display">
\begin{split}
\frac{\partial J_M}{\partial z_{in}}&=-\frac{2}{N}(x_n-\tilde x_n)^Tb_i=-\frac{2}{N}\left(x_n-\sum_{m=1}^{M}z_{mn}b_m\right)^Tb_i\\
&\overset{\text{ONB}}{=}-\frac{2}{N}(x_n^Tb_i-z_{in}b_i^Tb_i)=-\frac{2}{N}(x_n^Tb_i-z_{in})
\end{split}</script><p>since $b^⊤_i b_i = 1$. Setting this partial derivative to $0$ yields immediately the</p>
<script type="math/tex; mode=display">
z_{in}=x_n^Tb_i=b_i^Tx_n</script><p>for $i = 1, . . . ,M$ and $n = 1, . . . ,N$. This means that the optimal coordinates $z_{in}$ of the projection $\tilde xn$ are the coordinates of the orthogonal projection of the original data point $x_n$ onto the one-dimensional subspace that is spanned by $b_i$. Consequently:</p>
<ul>
<li>The optimal linear projection $\tilde x_n$ of $x_n$ is an orthogonal projection.</li>
<li>The coordinates of $\tilde x_n$with respect to the basis $(b_1, . . . , b_M)$ are the coordinates of the orthogonal projection of $x_n$ onto the principal subspace.</li>
<li>An orthogonal projection is the best linear mapping given the objective.</li>
<li>The coordinates $\zeta_m$ of $x$ and the coordinates $z_m$ of $\tilde x_n$ must be identical for $m = 1, . . . ,M$ since $U^⊥ =\text{span}[b_{M+1}, . . . , b_D]$ is the orthogonal complement of $U = \text{span}[b_1, . . . , b_M]$.</li>
</ul>
</li>
<li><p>If $(b_1, . . . , b_D)$ is an orthonormal basis of $\mathbb R^D$ then</p>
<script type="math/tex; mode=display">
\tilde x=b_j(b_j^Tb_j)^{-1}b_j^Tx=b_jb_j^Tx\in\mathbb R^D</script><p>is the orthogonal projection of $x$ onto the subspace spanned by the $j$th basis vector, and $z_j = b^⊤_j x $is the coordinate of this projection with respect to the basis vector $b_j$ that spans that subspace since $z_jb_j = \tilde x$. If we aim to project onto an $M$-dimensional subspace of $\mathbb R^D$, we obtain the orthogonal projection of $x$ onto the $M$-dimensional subspace with orthonormal basis vectors $b_1, . . . , b_M$ as</p>
<script type="math/tex; mode=display">
\tilde x=B(\underset{=I}{\underbrace{B^TB}})^{-1}B^Tx=BB^Tx</script><p>where we defined $B := [b_1, . . . , b_M] ∈\mathbb R^{D×M}$. The coordinates of this projection with respect to the ordered basis $(b_1, . . . , b_M)$ are $z := B^⊤x$.</p>
</li>
<li><p>We can think of the coordinates as a representation of the projected vector in a new coordinate system defined by $(b_1, . . . , b_M)$. Note that although $\tilde x ∈\mathbb R^D$, we only need M coordinates $z_1, . . . , z_M$ to represent this vector; the other $D−M$ coordinates with respect to the basis vectors $(b_{M+1}, . . . , b_D)$ are always $0$.</p>
</li>
</ul>
<h3 id="Finding-the-Basis-of-the-Principal-Subspace"><a href="#Finding-the-Basis-of-the-Principal-Subspace" class="headerlink" title="Finding the Basis of the Principal Subspace"></a>Finding the Basis of the Principal Subspace</h3><ul>
<li><p>To reformulate the loss function, we exploit our results from before and obtain</p>
<script type="math/tex; mode=display">
\tilde x_n=\sum_{m=1}^Mz_{mn}b_m=\sum_{m=1}^M(x_n^Tb_m)b_m</script><p>We now exploit the symmetry of the dot product, which yields</p>
<script type="math/tex; mode=display">
\tilde x_n=\left(\sum_{m=1}^Mb_mb_m^T\right)x_n</script><p>Since we can generally write the original data point $x_n$ as a linear combination of all basis vectors, it holds that</p>
<script type="math/tex; mode=display">
\begin{split}
x_n&=\sum_{d=1}^Dz_{dn}b_d=\sum_{d=1}^D(x_n^Tb_d)b_d=\left(\sum_{d=1}^Db_db_d^T\right)x_n\\
&=\left(\sum_{m=1}^Mb_mb_m^T\right)x_n+\left(\sum_{j=M+1}^Db_jb_j^T\right)x_n
\end{split}</script><p>where we split the sum with $D$ terms into a sum over M and a sum over $D −M$ terms. With this result, we find that the displacement vector $x_n −\tilde x_n$, i.e., the difference vector between the original data point and its projection, is</p>
<script type="math/tex; mode=display">
\begin{split}
x_n-\tilde x_n&=\left(\sum_{j=M+1}^Db_jb_j^T\right)x_n\\
&=\sum_{j=M+1}^D(x_n^Tb_j)b_j
\end{split}</script><p>This means the difference is exactly the projection of the data point onto the orthogonal complement of the principal subspace: We identify the matrix $\sum_{j=M+1}^Db_jb_j^T$  as the projection matrix that performs this projection. Hence the displacement vector $x_n − \tilde x_n$ lies in the subspace that is orthogonal to the principal subspace.</p>
</li>
<li><p><strong>Low-Rank Approximation</strong>: We saw that the projection matrix, which projects $x$ onto $\tilde x$, is given by</p>
<script type="math/tex; mode=display">
\sum_{m=1}^Mb_mb_m^T=BB^T</script><p>By construction as a sum of rank-one matrices $b_mb_m^⊤$ we see that $BB^⊤$ is symmetric and has rank $M$. Therefore, the average squared reconstruction error can also be written as</p>
<script type="math/tex; mode=display">
\begin{split}
&\frac{1}{N}\sum_{n=1}^N\left\|x_n-\tilde x_n\right\|^2=\frac{1}{N}\sum_{n=1}^N\left\|x_n-BB^Tx_n\right\|^2\\
&=\frac{1}{N}\sum_{n=1}^N\left\|(I-BB^T)x_n\right\|^2
\end{split}</script><p>Finding orthonormal basis vectors $b_1, . . . , b_M,$ which minimize the difference between the original data $x_n$ and their projections $\tilde x_n$, is equivalent to finding the best rank-$M$ approximation $BB^⊤$ of the identity matrix $I$.</p>
</li>
<li><p>Now we have all the tools to reformulate the loss function</p>
<script type="math/tex; mode=display">
J_M=\frac{1}{N}\sum_{n=1}^N\left\|x_n-\tilde x_n\right\|^2=\frac{1}{N}\sum_{n=1}^N\left\|\sum_{j=M+1}^D(b_j^Tx_n)b_j\right\|^2</script><p>We now explicitly compute the squared norm and exploit the fact that the $b_j$ form an ONB, which yields</p>
<script type="math/tex; mode=display">
\begin{split}
J_M&=\frac{1}{N}\sum_{n=1}^N\sum_{j=M+1}^D(b_j^Tx_n)^2=\frac{1}{N}\sum_{n=1}^N\sum_{j=M+1}^Db_j^Tx_nb_j^Tx_n\\
&=\frac{1}{N}\sum_{n=1}^N\sum_{j=M+1}^Db_j^Tx_nx_n^Tb_j
\end{split}</script><p>where we exploited the symmetry of the dot product in the last step to write $b^⊤_j x_n = x^⊤_n b_j$. We now swap the sums and obtain</p>
<script type="math/tex; mode=display">
\begin{split}
J_M&=\sum_{j=M+1}^Db_j^T\underset{=:S}{\underbrace{\left(\frac{1}{N}\sum_{n=1}^Nx_nx_n^T\right)}}b_j=\sum_{j=M+1}^Db_j^TSb_j\\
&=\sum_{j=M+1}^Dtr(b_j^TSb_j)=\sum_{j=M+1}^Dtr(Sb_jb_j^T)=tr(\underset{\text{projection matrix}}{\underbrace{\left(\sum_{j=M+1}^Db_jb_j^T\right)}}S)
\end{split}</script><p>where we exploited the property that the trace operator $tr(·)$ is linear and invariant to cyclic permutations of its arguments. Since wea ssumed that our dataset is centered, i.e., $\mathbb E[X] = 0$, we identify $S$ as the data covariance matrix. Since the projection matrix is constructed as a sum of rank-one matrices $b_jb^⊤_j$ it itself is of rank $D − M$. Equation implies that we can formulate the average squared reconstruction error equivalently as the covariance matrix of the data, projected onto the orthogonal complement of the principal subspace.</p>
</li>
<li><p>The average squared reconstruction error, when projecting onto the $M$-dimensional principal subspace, is</p>
<script type="math/tex; mode=display">
J_M=\sum_{j=M+1}^D\lambda_j</script><p>where $λ_j$ are the eigenvalues of the data covariance matrix. Therefore, to minimize average squared reconstruction error we need to select the smallest $D −M$ eigenvalues, which then implies that their corresponding eigenvectors are the basis of the orthogonal complement of the principal subspace.</p>
</li>
</ul>
<h2 id="Eigenvector-Computation-and-Low-Rank-Approximations"><a href="#Eigenvector-Computation-and-Low-Rank-Approximations" class="headerlink" title="Eigenvector Computation and Low-Rank Approximations"></a>Eigenvector Computation and Low-Rank Approximations</h2><ul>
<li><p>We obtained the basis of the principal subspace as the eigenvectors that are associated with the largest eigenvalues of the data covariance matrix</p>
<script type="math/tex; mode=display">
\begin{split}
S&=\frac{1}{N}\sum_{n=1}^Nx_nx_n^T=\frac{1}{N}XX^T\\
X&=[x_1,...,x_N]\in\mathbb R^{D\times N}
\end{split}</script><p>Note that $X$ is a $D × N$ matrix, i.e., it is the transpose of the “typical” data matrix (. To get the eigenvalues (and the corresponding eigenvectors) of $S$, we can follow two approaches:</p>
<ul>
<li>We perform an eigendecomposition and compute the eigenvalues and eigenvectors of $S$ directly.</li>
<li>We use a singular value decomposition. Since $S$ is symmetric and factorizes into $XX^⊤$ (ignoring the factor $\frac{1}{N}$ ), the eigenvalues of $S$ are the squared singular values of $X$.</li>
</ul>
</li>
<li><p>More specifically, the SVD of $X$ is given by</p>
<script type="math/tex; mode=display">
\underset{D\times N}{\underbrace{X}}=\underset{D\times D}{\underbrace{U}}\underset{D\times N}{\underbrace{\varSigma}}\underset{N\times N}{\underbrace{V^T}}</script><p>where $U ∈\mathbb R^{D×D}$ and $V^⊤ ∈\mathbb R^{N×N}$ are orthogonal matrices and $Σ ∈\mathbb R^{D×N}$ is a matrix whose only nonzero entries are the singular values $σ_{ii} ⩾0$. It then follows that</p>
<script type="math/tex; mode=display">
S=\frac{1}{N}XX^T=\frac{1}{N}U\varSigma\underset{=I_N}{\underbrace{V^TV}}\varSigma^TU^T=\frac{1}{N}U\varSigma\varSigma^TU^T</script><p>we get that the columns of $U$ are the eigenvectors of $XX^⊤$ (and therefore $S$). Furthermore, the eigenvalues $λ_d$ of $S$ are related to the singular values of $X$ via</p>
<script type="math/tex; mode=display">
\lambda_d=\frac{\sigma^2_d}{N}</script><p>This relationship between the eigenvalues of $S$ and the singular values of $X$ provides the connection between the maximum variance view and the singular value decomposition.</p>
</li>
</ul>
<h3 id="PCA-Using-Low-Rank-Matrix-Approximations"><a href="#PCA-Using-Low-Rank-Matrix-Approximations" class="headerlink" title="PCA Using Low-Rank Matrix Approximations"></a>PCA Using Low-Rank Matrix Approximations</h3><ul>
<li><p>To maximize the variance of the projected data (or minimize the average squared reconstruction error), PCA chooses the columns of $U$  to be the eigenvectors that are associated with the $M$ largest eigenvalues of the data covariance matrix $S$ so that we identify $U$ as the projection matrix $B$ , which projects the original data onto a lower-dimensional subspace of dimension $M$.</p>
</li>
<li><p>The Eckart-Young theorem offers a direct way to estimate the low-dimensional representation. Consider the best rank-$M$ approximation</p>
<script type="math/tex; mode=display">
\tilde X_M:=\text{argmin}_{rk(A)\le M}\left\|X-A\right\|_2\in\mathbb R^{D\times N}</script><p>of $X$, where $∥·∥_2$ is the spectral norm. The Eckart-Young theorem states that $\tilde X_M$ is given by truncating the SVD at the top-$M$ singular value. In other words, we obtain</p>
<script type="math/tex; mode=display">
\tilde X_M=\underset{D\times M}{\underbrace{U_M}}\underset{M\times M}{\underbrace{\varSigma_M}}\underset{M\times N}{\underbrace{V_M^T}}\in\mathbb R^{D\times N}</script><p>with orthogonal matrices $U_M := [u_1, . . . , u_M] ∈\mathbb R^{D\times M}$ and $V_M :=[v_1, . . . , v_M] ∈\mathbb R^{N×M}$ and a diagonal matrix $Σ_M ∈\mathbb R^{M×M}$ whose diagonal entries are the $M$ largest singular values of $X$.</p>
</li>
</ul>
<h3 id="Practical-Aspects"><a href="#Practical-Aspects" class="headerlink" title="Practical Aspects"></a>Practical Aspects</h3><ul>
<li><p>The Abel-Ruffini theorem states that there exists no algebraic solution to this problem for polynomials of degree $5$ or more. Therefore, in practice, we solve for eigenvalues or singular values using iterative methods, which are implemented in all modern packages for linear algebra.</p>
</li>
<li><p>In many applications (such as PCA presented in this chapter), we only require a few eigenvectors. It would be wasteful to compute the full decomposition, and then discard all eigenvectors with eigenvalues that are beyond the first few.</p>
</li>
<li><p>In the extreme case of only needing the first eigenvector, a simple method called the <strong><em>power iteration</em></strong> is very efficient. Power iteration chooses a random vector $x_0$ that is not in  the null space of $S$ and follows the iteration</p>
<script type="math/tex; mode=display">
x_{k+1}=\frac{S_{x_k}}{\left\|S_{x_k}\right\|},k=0,1,...</script><p>This means the vector $x_k$ is multiplied by $S$ in every iteration and then normalized, i.e., we always have $∥x_k∥ = 1$.</p>
</li>
</ul>
<h2 id="PCA-in-High-Dimensions"><a href="#PCA-in-High-Dimensions" class="headerlink" title="PCA in High Dimensions"></a>PCA in High Dimensions</h2><ul>
<li><p>In order to do PCA, we need to compute the data covariance matrix. In $D$ dimensions, the data covariance matrix is a $D×D$ matrix. Computing the eigenvalues and eigenvectors of this matrix is computationally expensive as it scales cubically in $D$.</p>
</li>
<li><p>Assume we have a centered dataset $x_1, . . . , x_N, x_n ∈\mathbb R^D$. Then the data covariance matrix is given as</p>
<script type="math/tex; mode=display">
S=\frac{1}{N}XX^T\in\mathbb R^{D\times D}</script><p>where $X = [x_1, . . . , x_N]$ is a $D ×N$ matrix whose columns are the data points.</p>
</li>
<li><p>We now assume that $N ≪ D$, i.e., the number of data points is smaller than the dimensionality of the data. If there are no duplicate data points, the rank of the covariance matrix $S$ is $N$, so it has $D−N+1$ many eigenvalues that are $0$. Intuitively, this means that there are some redundancies.</p>
</li>
<li><p>In the following, we will exploit this and turn the $D×D$ covariance matrix into an $N ×N$ covariance matrix whose eigenvalues are all positive.</p>
</li>
<li><p>In PCA, we ended up with the eigenvector equation</p>
<script type="math/tex; mode=display">
Sb_m=\lambda_mb_m,\quad m=1,....,N</script><p>where $b_m$ is a basis vector of the principal subspace. Let us rewrite this equation a bit: With $S$, we obtain</p>
<script type="math/tex; mode=display">
Sb_m=\frac{1}{N}XX^Tb_m=\lambda_mb_m</script><p>We now multiply $X^⊤ ∈\mathbb R^{N×D}$ from the left-hand side, which yields</p>
<script type="math/tex; mode=display">
\frac{1}{N}\underset{N\times N}{\underbrace{X^TX}}\underset{=:c_m}{\underbrace{X^Tb_m}}=\lambda_mX^Tb_m\Leftrightarrow \frac{1}{N}X^TXc_m=\lambda_mc_m</script><p>and we get a new eigenvector/eigenvalue equation: $λ_m$ remains eigenvalue, which confirms our results that the nonzero eigenvalues of $XX^⊤$ equal the nonzero eigenvalues of $X^⊤X$.</p>
</li>
<li><p>We obtain the eigenvector of the matrix $\frac{1}{N}X^TX\in\mathbb R^{N\times N}$ associated with $λ_m$ as $c_m := X^⊤b_m$. Assuming we have no duplicate data points, this matrix has rank $N$ and is invertible. This also implies that $\frac{1}{N}X^TX$ has the same (nonzero) eigenvalues as the data covariance matrix $S$. But this is now an $N ×N$ matrix, so that we can compute the eigenvalues and eigenvectors much more efficiently than for the original $D×D$ data covariance matrix.</p>
</li>
<li><p>Now that we have the eigenvectors of $\frac{1}{N}X^TX$, we are going to recover the original eigenvectors, which we still need for PCA. Currently, we know the eigenvectors of $\frac{1}{N}X^TX$. If we left-multiply our eigenvalue/eigenvector equation with $X$, we get</p>
<script type="math/tex; mode=display">
\underset{S}{\underbrace{\frac{1}{N}X^TX}}Xc_m=\lambda_mXc_m</script><p>and we recover the data covariance matrix again. This now also means that we recover $Xc_m$ as an eigenvector of $S$.</p>
</li>
</ul>
<h2 id="Key-Steps-of-PCA-in-Practice"><a href="#Key-Steps-of-PCA-in-Practice" class="headerlink" title="Key Steps of PCA in Practice"></a>Key Steps of PCA in Practice</h2><p>In the following, we will go through the individual steps of PCA using a running example, which is summarized in Figure. We are given a two-dimensional dataset (Figure(a)), and we want to use PCA to project it onto a one-dimensional subspace.<br><img src="https://cdn.jsdelivr.net/gh/Eliauk-L/blog-img@img/img/202408111051451.png" alt="Steps of PCA."></p>
<ol>
<li><p><strong>Mean subtraction</strong>. We start by centering the data by computing the mean $\mu$ of the dataset and subtracting it from every single data point. This ensures that the dataset has mean $0$ (Figure(b)). Mean subtraction is not strictly necessary but reduces the risk of numerical problems.</p>
</li>
<li><p><strong>Standardization</strong>. Divide the data points by the standard deviation $σ_d$ of the dataset for every dimension $d = 1, . . . ,D$. Now the data is unit free, and it has variance $1$ along each axis, which is indicated by the two arrows in Figure(c). This step completes the <em>standardization</em> of the data.</p>
</li>
<li><p><strong>Eigendecomposition of the covariance matrix</strong>. Compute the data covariance matrix and its eigenvalues and corresponding eigenvectors. Since the covariance matrix is symmetric, the spectral theorem states that we can find an ONB of eigenvectors. In Figure(d), the eigenvectors are scaled by the magnitude of the corresponding eigenvalue. The longer vector spans the principal subspace, which we denote by $U$. The data covariance matrix is represented by the ellipse.</p>
</li>
<li><p><strong>Projection</strong>. We can project any data point $x_∗ ∈\mathbb R^D$ onto the principal subspace: To get this right, we need to standardize $x_∗$ using the mean $\mu_d$ and standard deviation $σ_d$ of the training data in the dth dimension, respectively, so that</p>
<script type="math/tex; mode=display">
x_*^{(d)}\leftarrow\frac{x_*^{(d)}-\mu_d}{\sigma_d},\quad d=1,...,D</script><p>where $x^{(d)}_∗$ is the $d$th component of $x_∗$. We obtain the projection as</p>
<script type="math/tex; mode=display">
\tilde x_*=BB^Tx_*</script><p>with coordinates</p>
<script type="math/tex; mode=display">
z_*=B^Tx_*</script><p>with respect to the basis of the principal subspace. Here, $B$ is the matrix that contains the eigenvectors that are associated with the largest eigenvalues of the data covariance matrix as columns. PCA returns the coordinates, not the projections $x_∗$.<br>Having standardized our dataset, projection only yields the projections in the context of the standardized dataset. To obtain our projection in the original data space (i.e., before standardization), we need to undo the standardization and multiply by the standard deviation before adding the mean so that we obtain</p>
<script type="math/tex; mode=display">
\tilde x_*^{(d)}\leftarrow\tilde x_*\sigma_d+\mu_d,\quad d=1,...,D</script></li>
</ol>
<h2 id="Latent-Variable-Perspective"><a href="#Latent-Variable-Perspective" class="headerlink" title="Latent Variable Perspective"></a>Latent Variable Perspective</h2><ul>
<li>In the previous sections, we derived PCA without any notion of a probabilistic model using the maximum-variance and the projection perspectives. On the one hand, this approach may be appealing as it allows us to sidestep all the mathematical difficulties that come with probability theory, but on the other hand, a probabilistic model would offer us more flexibility and useful insights. More specifically, a probabilistic model would<ul>
<li>Come with a likelihood function, and we can explicitly deal with noisy observations (which we did not even discuss earlier)</li>
<li>Allow us to do Bayesian model comparison via the marginal likelihood</li>
<li>View PCA as a generative model, which allows us to simulate new data</li>
<li>Allow us to make straightforward connections to related algorithms</li>
<li>Deal with data dimensions that are missing at random by applying Bayes’ theorem</li>
<li>Give us a notion of the novelty of a new data point</li>
<li>Give us a principled way to extend the model, e.g., to a mixture of PCA models</li>
<li>Have the PCA we derived in earlier sections as a special case</li>
<li>Allow for a fully Bayesian treatment by marginalizing out the model parameters</li>
</ul>
</li>
<li>By introducing a continuous-valued latent variable $z ∈\mathbb R^M$ it is possible to phrase PCA as a probabilistic latent-variable model. </li>
<li>Tipping and Bishop (1999) proposed this latent-variable model as <strong><em>probabilistic PCA (PPCA)</em></strong>.</li>
</ul>
<h3 id="Generative-Process-and-Probabilistic-Model"><a href="#Generative-Process-and-Probabilistic-Model" class="headerlink" title="Generative Process and Probabilistic Model"></a>Generative Process and Probabilistic Model</h3><ul>
<li><p>In PPCA, we explicitly write down the probabilistic model for linear dimensionality reduction. For this we assume a continuous latent variable $z ∈\mathbb R^M$ with a standard-normal prior $p(z) =\mathcal N(0, I)$ and a linear relationship between the latent variables and the observed $x$ data where</p>
<script type="math/tex; mode=display">
x=Bz+\mu+\epsilon\in\mathbb R^D</script><p>where $\epsilon\sim\mathcal N(0,\sigma^2I)$ is Gaussian observation noise and $B ∈\mathbb R^{D×M}$  and $\mu ∈\mathbb R^D$ describe the linear/affine mapping from latent to observed variables. Therefore, PPCA links latent and observed variables via</p>
<script type="math/tex; mode=display">
p(x|z,B,\mu,\sigma^2)=\mathcal N(x|Bz+\mu,\sigma^2I)</script><p>Overall, PPCA induces the following generative process:</p>
<script type="math/tex; mode=display">
\begin{split}
z_n&\sim \mathcal N(z|0,I)\\
x_n|z_n&\sim \mathcal N(x|Bz+\mu,\sigma^2I)
\end{split}</script></li>
<li><p>To generate a data point that is typical given the model parameters, we follow an <em>ancestral sampling scheme</em>: We first sample a latent variable $z_n$ from $p(z)$. Then we use $z_n$ to sample a data point conditioned on the sampled $z_n$, i.e., $x_n\sim p(x|z_n,B,\mu,\sigma^2)$.</p>
</li>
<li><p>This generative process allows us to write down the probabilistic model (i.e., the joint distribution of all random variables) as</p>
<script type="math/tex; mode=display">
p(x,z|B,\mu,\sigma^2)=p(x|z,B,\mu,\sigma^2)p(z)</script><p><img src="https://cdn.jsdelivr.net/gh/Eliauk-L/blog-img@img/img/202408111052892.png" alt="Graphical model for probabilistic PCA."></p>
</li>
</ul>
<h3 id="Likelihood-and-Joint-Distribution"><a href="#Likelihood-and-Joint-Distribution" class="headerlink" title="Likelihood and Joint Distribution"></a>Likelihood and Joint Distribution</h3><ul>
<li><p>We obtain the likelihood of this probabilistic model by integrating out the latent variable $z$ so that</p>
<script type="math/tex; mode=display">
\begin{split}
p(x|B,\mu,\sigma^2)&=\int p(x|z,B,\mu,\sigma^2)p(z)dz\\
&=\int \mathcal N(x|Bz+\mu,\sigma^2I)\mathcal N(z|0,I)dz
\end{split}</script><p>We know that the solution to this integral is a Gaussian distribution with mean</p>
<script type="math/tex; mode=display">
\mathbb E_x[x]=\mathbb E_z[Bz+\mu]+\mathbb E_\epsilon[\epsilon]=\mu</script><p>and with covariance matrix</p>
<script type="math/tex; mode=display">
\begin{split}
\mathbb V[x]&=\mathbb V_z[Bz+\mu]+\mathbb V_\epsilon[\epsilon]=\mathbb V_z[Bz]+\sigma^2I\\
&=B\mathbb V_z[z]B^T+\sigma^2I=BB^T+\sigma^2I
\end{split}</script><p>The likelihood can be used for maximum likelihood or MAP estimation of the model parameters.</p>
</li>
<li><p>We cannot use the conditional distribution for maximum likelihood estimation as it still depends on the latent variables. The likelihood function we require for maximum likelihood (or MAP) estimation should only be a function of the data $x$ and the model parameters, but must not depend on the latent variables.</p>
<p>We know that a Gaussian random variable $z$ and a linear/affine transformation $x = Bz$ of it are jointly Gaussian distributed. We already know the marginals $p(z) =\mathcal N(z | 0, I)$ and $p(x) =\mathcal N(x |\mu, BB^⊤ + σ^2I)$. The missing cross-covariance is given as</p>
<script type="math/tex; mode=display">
p(x,z|B,\mu,\sigma^2)=\mathcal N(\left[\begin{matrix}x\\z\end{matrix}\right]|\left[\begin{matrix}\mu\\0\end{matrix}\right],\left[\begin{matrix}BB^T+\sigma^2I&B\\B^T&I\end{matrix}\right])</script><p>with a mean vector of length $D + M$ and a covariance matrix of size $(D +M) × (D +M)$.</p>
</li>
</ul>
<h3 id="Posterior-Distribution"><a href="#Posterior-Distribution" class="headerlink" title="Posterior Distribution"></a>Posterior Distribution</h3><ul>
<li><p>The joint Gaussian distribution $p(x, z |B, \mu, σ^2)$  allows us to determine the posterior distribution $p(z | x)$ immediately by applying the rules of Gaussian conditioning. The posterior distribution of the latent variable given an observation $x$ is then</p>
<script type="math/tex; mode=display">
\begin{split}
p(z|x)&=\mathcal N(z|m,C)\\
m&=B^T(BB^T+\sigma^2I)^{-1}(x-\mu)\\
C&=I-B^T(BB^T+\sigma^2I)^{-1}B
\end{split}</script><p>Note that the posterior covariance does not depend on the observed data $x$. For a new observation $x_∗$ in data space, we use $p(z|x)$ to determine the posterior distribution of the corresponding latent variable $z_∗$.</p>
</li>
<li><p>We can explore the posterior distribution to understand what other data points x are plausible under this posterior. To do this, we exploit the generative process underlying PPCA, which allows us to explore the posterior distribution on the latent variables by generating new data that is plausible under this posterior:</p>
<ul>
<li>Sample a latent variable $z_∗ ∼ p(z | x_∗)$ from the posterior distribution over the latent variables</li>
<li>Sample a reconstructed vector $\tilde x_∗ ∼ p(x | z_∗,B, \mu, σ^2)$</li>
</ul>
<p>If we repeat this process many times, we can explore the posterior distribution on the latent variables $z_∗$ and its implications on the observed data.</p>
</li>
</ul>
<blockquote>
<p><strong>Bibliography:</strong></p>
<ol>
<li>Mathematics for Machine Learning_Marc Peter Deisenroth_2020</li>
</ol>
</blockquote>

                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        文章作者:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/about" rel="external nofollow noreferrer">Jay</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        文章链接:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="http://Eliauk-L.github.io/2024/08/11/dimensionalityreductionwithprincipalcomponentanalysis/">http://Eliauk-L.github.io/2024/08/11/dimensionalityreductionwithprincipalcomponentanalysis/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        版权声明:
                    </i>
                </span>
                <span class="reprint-info">
                    本博客所有文章除特別声明外，均采用
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    许可协议。转载请注明来源
                    <a href="/about" target="_blank">Jay</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>复制成功，请遵循本文的转载规则</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">查看</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/tags/Mathematics/">
                                    <span class="chip bg-color">Mathematics</span>
                                </a>
                            
                                <a href="/tags/%E7%AC%94%E8%AE%B0/">
                                    <span class="chip bg-color">笔记</span>
                                </a>
                            
                                <a href="/tags/MachineLearing/">
                                    <span class="chip bg-color">MachineLearing</span>
                                </a>
                            
                                <a href="/tags/DimensionalityReduction/">
                                    <span class="chip bg-color">DimensionalityReduction</span>
                                </a>
                            
                                <a href="/tags/PCA/">
                                    <span class="chip bg-color">PCA</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="qq, qzone, wechat, weibo, douban" data-wechat-qrcode-helper="<p>微信扫一扫即可分享！</p>"></div>
    <script src="/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;上一篇</div>
            <div class="card">
                <a href="/2024/08/12/javaweb-si-wei-dao-tu/">
                    <div class="card-image">
                        
                        
                        <img src="/medias/featureimages/0.jpg" class="responsive-img" alt="JavaWeb思维导图">
                        
                        <span class="card-title">JavaWeb思维导图</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2024-08-12
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/categories/Java/" class="post-category">
                                    Java
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/tags/JavaWeb/">
                        <span class="chip bg-color">JavaWeb</span>
                    </a>
                    
                    <a href="/tags/%E6%80%9D%E7%BB%B4%E5%AF%BC%E5%9B%BE/">
                        <span class="chip bg-color">思维导图</span>
                    </a>
                    
                    <a href="/tags/HTML/">
                        <span class="chip bg-color">HTML</span>
                    </a>
                    
                    <a href="/tags/CSS/">
                        <span class="chip bg-color">CSS</span>
                    </a>
                    
                    <a href="/tags/JavaScript/">
                        <span class="chip bg-color">JavaScript</span>
                    </a>
                    
                    <a href="/tags/JSP/">
                        <span class="chip bg-color">JSP</span>
                    </a>
                    
                    <a href="/tags/Servlet/">
                        <span class="chip bg-color">Servlet</span>
                    </a>
                    
                    <a href="/tags/Tomcat/">
                        <span class="chip bg-color">Tomcat</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                下一篇&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/2024/08/10/linearregression/">
                    <div class="card-image">
                        
                        
                        <img src="/medias/featureimages/13.jpg" class="responsive-img" alt="Linear Regression">
                        
                        <span class="card-title">Linear Regression</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2024-08-10
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/categories/MachineLearing/" class="post-category">
                                    MachineLearing
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/tags/Mathematics/">
                        <span class="chip bg-color">Mathematics</span>
                    </a>
                    
                    <a href="/tags/%E7%AC%94%E8%AE%B0/">
                        <span class="chip bg-color">笔记</span>
                    </a>
                    
                    <a href="/tags/MachineLearing/">
                        <span class="chip bg-color">MachineLearing</span>
                    </a>
                    
                    <a href="/tags/LinearRegression/">
                        <span class="chip bg-color">LinearRegression</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- 代码块功能依赖 -->
<script type="text/javascript" src="/libs/codeBlock/codeBlockFuction.js"></script>


  <!-- 是否加载使用自带的 prismjs. -->
  <script type="text/javascript" src="/libs/prism/prism.min.js"></script>


<!-- 代码语言 -->

<script type="text/javascript" src="/libs/codeBlock/codeLang.js"></script>


<!-- 代码块复制 -->

<script type="text/javascript" src="/libs/codeBlock/codeCopy.js"></script>


<!-- 代码块收缩 -->

<script type="text/javascript" src="/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;目录</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC 悬浮按钮. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* 修复文章卡片 div 的宽度. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // 切换TOC目录展开收缩的相关操作.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>

    

</main>


<script src="https://cdn.bootcss.com/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
<script>
    MathJax.Hub.Config({
        tex2jax: {inlineMath: [['$', '$'], ['\\(', '\\)']]}
    });
</script>



    <footer class="page-footer bg-color">
    

    <div class="container row center-align"
         style="margin-bottom: 0px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024</span>
            
            <a href="/about" target="_blank">Jay</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;总访问量:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;总访问人数:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- 运行天数提醒. -->
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Eliauk-L" class="tooltipped" target="_blank" data-tooltip="访问我的GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:2571368706@qq.com" class="tooltipped" target="_blank" data-tooltip="邮件联系我" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>







    <a href="tencent://AddContact/?fromId=50&fromSubId=1&subcmd=all&uin=2571368706" class="tooltipped" target="_blank" data-tooltip="QQ联系我: 2571368706" data-position="top" data-delay="50">
        <i class="fab fa-qq"></i>
    </a>







</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- 搜索遮罩框 -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;搜索</span>
            <input type="search" id="searchInput" name="s" placeholder="请输入搜索的关键字"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- 白天和黑夜主题 -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- 回到顶部按钮 -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/libs/materialize/materialize.min.js"></script>
    <script src="/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/libs/aos/aos.js"></script>
    <script src="/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/js/matery.js"></script>

    

    
    
    

    <!-- 雪花特效 -->
     
        <script type="text/javascript">
            // 只在桌面版网页启用特效
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/libs/others/snow.js"><\/script>');
            }
        </script>
    

    <!-- 鼠标星星特效 -->
    

    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--腾讯兔小巢-->
    
    

    

    

    
    <script src="/libs/instantpage/instantpage.js" type="module"></script>
    

</body>

</html>
