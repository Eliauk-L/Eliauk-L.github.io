<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="Continuous Optimization, Jay&#39;s Blog">
    <meta name="description" content="Continuous OptimizationOptimization Using Gradient Descent
We now consider the problem of solving for the minimum of a r">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>Continuous Optimization | Jay&#39;s Blog</title>
    <link rel="icon" type="image/png" href="/favicon.png">
    


    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/css/matery.css">
<link rel="stylesheet" type="text/css" href="/css/my.css">
<link rel="stylesheet" type="text/css" href="/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/css/post.css">




    



    <script src="/libs/jquery/jquery-3.6.0.min.js"></script>

    <script src="https://sdk.jinrishici.com/v2/browser/jinrishici.js" charset="utf-8"></script>

<meta name="generator" content="Hexo 7.2.0">
<style>.github-emoji { position: relative; display: inline-block; width: 1.2em; min-height: 1.2em; overflow: hidden; vertical-align: top; color: transparent; }  .github-emoji > span { position: relative; z-index: 10; }  .github-emoji img, .github-emoji .fancybox { margin: 0 !important; padding: 0 !important; border: none !important; outline: none !important; text-decoration: none !important; user-select: none !important; cursor: auto !important; }  .github-emoji img { height: 1.2em !important; width: 1.2em !important; position: absolute !important; left: 50% !important; top: 50% !important; transform: translate(-50%, -50%) !important; user-select: none !important; cursor: auto !important; } .github-emoji-fallback { color: inherit; } .github-emoji-fallback img { opacity: 0 !important; }</style>
</head>


<body>
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/" class="waves-effect waves-light">
                    
                    <span class="logo-span">Jay&#39;s Blog</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>首页</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>标签</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>分类</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>归档</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/about" class="waves-effect waves-light">
      
      <i class="fas fa-user-circle" style="zoom: 0.6;"></i>
      
      <span>关于</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/galleries" class="waves-effect waves-light">
      
      <i class="fas fa-image" style="zoom: 0.6;"></i>
      
      <span>相册</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="搜索" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="深色/浅色模式" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <div class="logo-name">Jay&#39;s Blog</div>
        <div class="logo-desc">
            
            Jay&#39;s Blog
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			首页
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			标签
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			分类
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			归档
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/about" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-user-circle"></i>
			
			关于
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/galleries" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-image"></i>
			
			相册
		</a>
          
        </li>
        
        
    </ul>
</div>


        </div>

        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('/medias/featureimages/50.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">Continuous Optimization</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- 文章内容详情 -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/tags/Mathematics/">
                                <span class="chip bg-color">Mathematics</span>
                            </a>
                        
                            <a href="/tags/%E7%AC%94%E8%AE%B0/">
                                <span class="chip bg-color">笔记</span>
                            </a>
                        
                            <a href="/tags/AnalyticGeometry/">
                                <span class="chip bg-color">AnalyticGeometry</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/categories/Mathematics/" class="post-category">
                                Mathematics
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>发布日期:&nbsp;&nbsp;
                    2024-08-06
                </div>
                

                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>文章字数:&nbsp;&nbsp;
                    4.1k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>阅读时长:&nbsp;&nbsp;
                    25 分
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>阅读次数:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        
        <!-- 是否加载使用自带的 prismjs. -->
        <link rel="stylesheet" href="/libs/prism/prism.min.css">
        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <h1 id="Continuous-Optimization"><a href="#Continuous-Optimization" class="headerlink" title="Continuous Optimization"></a>Continuous Optimization</h1><h2 id="Optimization-Using-Gradient-Descent"><a href="#Optimization-Using-Gradient-Descent" class="headerlink" title="Optimization Using Gradient Descent"></a>Optimization Using Gradient Descent</h2><ul>
<li><p>We now consider the problem of solving for the minimum of a real-valued function</p>
<script type="math/tex; mode=display">
\underset{x}{\min}f(x)</script><p>where $f :\mathbb R^d →\mathbb R$ is an objective function that captures the machine learning problem at hand. We assume that our function $f$ is differentiable, and we are unable to analytically find a solution in closed form.</p>
</li>
<li><p><strong>Gradient descent is a first-order optimization algorithm</strong>. To find a local minimum of a function using gradient descent, one takes steps proportional to the negative of the gradient of the function at the current point. Recall that the gradient points in the direction of the steepest ascent. Another useful intuition is to consider the set of lines where the function is at a certain value ($f(x) = c$ for some value $c ∈\mathbb R$), which are known as the contour lines. The gradient points in a direction that is <strong>orthogonal to the contour lines</strong> of the function we wish to optimize.</p>
</li>
<li><p>Let us consider multivariate functions. Imagine a surface (described by the function $f(x)$) with a ball starting at a particular location $x_0$. When the ball is released, it will move downhill in the direction of steepest descent. Gradient descent exploits the fact that $f(x_0)$ decreases fastest if one moves from $x_0$ in the direction of the negative gradient $−((∇f)(x_0))^⊤$ of $f$ at $x_0$. Then, if</p>
<script type="math/tex; mode=display">
x_1=x_0-\gamma((\nabla f)(x_0))^T</script><p>for a small step-size $γ ⩾ 0$, then $f(x_1) ⩽ f(x_0)$. Note that we use the transpose for the gradient since otherwise the dimensions will not work out.</p>
</li>
<li><p>This observation allows us to define a simple <em>gradient descent algorithm</em>: If we want to find a local optimum $f(x_∗)$ of a function $f :\mathbb R^n →\mathbb R, x \mapsto f(x)$, we start with an initial guess $x_0$ of the parameters we wish to optimize and then iterate according to</p>
<script type="math/tex; mode=display">
x_{i+1}=x_{i}-\gamma_i((\nabla f)(x_i))^T</script><p>For suitable step-size $γ_i$, the sequence $f(x_0) ⩾ f(x_1) ⩾ . . . $converges to a local minimum.</p>
<blockquote>
<p>e.g.  Consider a quadratic function in two dimensions</p>
<script type="math/tex; mode=display">
f\left(\left[\begin{matrix}x_1\\x_2\end{matrix}\right]\right)=\frac{1}{2}\left[\begin{matrix}x_1\\x_2\end{matrix}\right]^T\left[\begin{matrix}2&1\\1&20\end{matrix}\right]\left[\begin{matrix}x_1\\x_2\end{matrix}\right]-\left[\begin{matrix}5\\3\end{matrix}\right]^T\left[\begin{matrix}x_1\\x_2\end{matrix}\right]</script><p>with gradient</p>
<script type="math/tex; mode=display">
\nabla f\left(\left[\begin{matrix}x_1\\x_2\end{matrix}\right]\right)=\left[\begin{matrix}x_1\\x_2\end{matrix}\right]^T\left[\begin{matrix}2&1\\1&20\end{matrix}\right]-\left[\begin{matrix}5\\3\end{matrix}\right]^T</script><p>Starting at the initial location $x_0 = [−3, −1]^⊤$, we iteratively apply gradient descent equation to obtain a sequence of estimates that converge to the minimum value. We can see (both from the figure and by plugging $x_0$ into gradient with $γ = 0.085$) that the negative gradient at $x_0$ points north and east, leading to $x_1 = [−1.98, 1.21]^⊤$. Repeating that argument gives us $x_2 = [−1.32, −0.42]^⊤$, and so on.</p>
<p><img src="https://cdn.jsdelivr.net/gh/Eliauk-L/blog-img@img/img/202408061138364.png" alt="Gradient descent on a two-dimensional quadratic surface"></p>
</blockquote>
</li>
<li><p>Gradient descent can be relatively slow close to the minimum: Its asymptotic rate of convergence is inferior to many other methods.</p>
</li>
<li><p>As mentioned earlier, choosing a good <em>step-size</em> is important in gradient descent. If the step-size is too small, gradient descent can be slow. If the step-size is chosen too large, gradient descent can overshoot, fail to converge, or even diverge. The step-size is also called the <em>learning rate</em>.</p>
</li>
<li><p>Adaptive gradient methods rescale the step-size at each iteration, depending on local properties of the function. There are two simple heuristics:</p>
<ul>
<li><em>When the function value increases after a gradient step, the step-size was too large. Undo the step and decrease the step-size.</em></li>
<li><em>When the function value decreases the step could have been larger. Try to increase the step-size.</em></li>
</ul>
</li>
<li><p>When applied to the solution of linear systems of equations $Ax = b$, gradient descent may converge slowly. The speed of convergence of gradient descent is dependent on the <strong><em>condition number</em></strong> $\mathcal k=\frac{\sigma(A)_{\max}}{\sigma(A_{\min})}$,  which <em>is the ratio of the maximum to the minimum singular value</em>of $A$. The condition number essentially measures the ratio of the most<br>curved direction versus the least curved direction. </p>
</li>
<li><p>Instead of directly solving $Ax = b$, one could instead solve $P^{−1}(Ax− b) = 0$, where<br>$P$ is called the <strong><em>preconditioner</em></strong>. The goal is to design $P^{−1}$ such that $P^{−1}A$ has a <em>better condition number</em>, but at the same time $P^{−1}$ is easy to compute.</p>
</li>
<li><p><strong><em>Gradient descent with momentum</em></strong> is a method that introduces an additional term to remember what happened in the previous iteration. This memory dampens oscillations and smoothes out the gradient updates. The momentum-based method remembers the update $∆x_i$ at each iteration $i$ and determines the next update as a linear combination of the current and previous gradients</p>
<script type="math/tex; mode=display">
\begin{split}
x_{i+1}&=x_i-\gamma_i((\nabla f)(x_i))^T+\alpha\varDelta x_i\\
\varDelta x_i&=x_i-x_{i-1}=\alpha \varDelta x_{i-1}-\gamma_{i-1}((\nabla f)(x_{i-1}))^T
\end{split}</script><p>where $α ∈ [0, 1]$. Sometimes we will only know the gradient approximately. In such cases, the momentum term is useful since it averages out different noisy estimates of the gradient.</p>
</li>
<li><p><strong><em>Stochastic gradient descent</em></strong> (often shortened as <strong>SGD</strong>) is a stochastic approximation of the gradient descent method for minimizing an objective function that is written as a sum of differentiable functions. The word stochastic here refers to the fact that we acknowledge that we do not know the gradient precisely, but instead only know a <em>noisy approximation</em> to it. By constraining the probability distribution of the approximate gradients, we can still theoretically guarantee that SGD will converge.</p>
</li>
<li><p>In machine learning, given $n = 1, . . . ,N$ data points, we often consider objective functions that are the sum of the losses $L_n$ incurred by each example $n$. In mathematical notation, we have the form</p>
<script type="math/tex; mode=display">
L(\theta)=\sum_{n=1}^N{L_n(\theta)}</script><p>where $θ$ is the vector of parameters of interest, i.e., we want to find $θ$ that minimizes $L$. An example from regression is the negative log-likelihood, which is expressed as a sum over log-likelihoods of individual examples so that</p>
<script type="math/tex; mode=display">
L(\theta)=-\sum_{n=1}^N{\log p(y_n|x_n,\theta)}</script><p>where $x_n ∈\mathbb R^D$ are the training inputs, $y_n$ are the training targets, and $θ$ are the parameters of the regression model.</p>
</li>
<li><p>Standard gradient descent, as introduced previously, is a “batch” opti-<br>mization method, i.e., optimization is performed using the full training set by updating the vector of parameters according to</p>
<script type="math/tex; mode=display">
\theta_{i+1}=\theta_i-\gamma_i(\nabla L(\theta_i))^T=\theta_i-\gamma_i\sum_{n=1}^N(\nabla L_n(\theta_i))^T</script><p>for a suitable step-size parameter $γ_i$. Evaluating the sum gradient may require expensive evaluations of the gradients from all individual functions $L_n$.<br>Consider the term $\sum_{n=1}^N(\nabla L_n(\theta_i))$. We can reduce the amount of computation by taking a sum over a smaller set of $L_n$. In contrast to batch gradient descent, which uses all $L_n$ for $n = 1, . . . ,N$, we randomly choose a subset of $L_n$ for mini-batch gradient descent. In the extreme case, we randomly select only a single $L_n$ to estimate the gradient. The key insight about why taking a subset of data is sensible is to realize that for gradient descent to converge, we only require that the gradient is an unbiased estimate of the true gradient. In fact the term $\sum_{n=1}^N(\nabla L_n(\theta_i))^T$ is an empirical estimate of the expected value of the gradient. Therefore, any other unbiased empirical estimate of the expected value, for example using any subsample of the data, would suffice for convergence of gradient descent.</p>
</li>
<li><p><em>When the learning rate decreases at an appropriate rate, and subject to relatively mild assumptions, stochastic gradient descent converges almost surely to local minimum.</em></p>
</li>
</ul>
<h2 id="Constrained-Optimization-and-Lagrange-Multipliers"><a href="#Constrained-Optimization-and-Lagrange-Multipliers" class="headerlink" title="Constrained Optimization and Lagrange Multipliers"></a>Constrained Optimization and Lagrange Multipliers</h2><ul>
<li><p>For real-valued functions $g_i :\mathbb R^D →\mathbb R$ for $i = 1, . . . ,m$, we consider the constrained<br>optimization problem</p>
<script type="math/tex; mode=display">
\underset{x}{\min}f(x)\\
\text{subject to}\quad g_i(x)\le0\quad\text{for all}\quad i=1,...,m</script><p>It is worth pointing out that the functions $f$ and $g_i$ could be non-convex in general. One obvious, but not very practical, way of converting the constrained problem into an unconstrained one is to use an indicator function</p>
<script type="math/tex; mode=display">
J(x)=f(X)+\sum_{i=1}^m{1({g_i(x)})}</script><p>where $1(z)$ is an infinite step function</p>
<script type="math/tex; mode=display">
1(z)=\begin{cases}
0\quad\text{if}\;z\le0\\
\infty\quad\text{otherwise}
\end{cases}</script><p>This gives infinite penalty if the constraint is not satisfied, and hence would provide the same solution. However, this infinite step function is equally difficult to optimize. We can overcome this difficulty by introducing <em>Lagrange multipliers</em>. The idea of Lagrange multipliers is to replace the step function with a linear function.</p>
<p>We associate to problem the Lagrangian by introducing the Lagrange multipliers $λ_i ⩾ 0 $corresponding to each inequality constraint respectively, so that</p>
<script type="math/tex; mode=display">
\begin{split}
\mathfrak L(x,\lambda)&=f(x)+\sum_{i=1}^m{\lambda_ig_i(x)}\\
&=f(x)+\lambda^Tg(x)
\end{split}</script><p>where in the last line we have concatenated all constraints $g_i(x)$ into a vector $g(x)$, and all the Lagrange multipliers into a vector $λ ∈\mathbb R^m$.</p>
</li>
<li><p>In general, duality in optimization is the idea of converting an optimization problem in one set of variables $x$ (called the primal variables), into another optimization problem in a different set of variables $λ$ (called the dual variables). We introduce two different approaches to duality: Lagrangian duality and Legendre-Fenchel duality.</p>
</li>
<li><p>The problem</p>
<script type="math/tex; mode=display">
\underset{x}{\min}f(x)\\
\text{subject to}\quad g_i(x)\le0\quad\text{for all}\quad i=1,...,m</script><p>is known as <em>the primal problem</em>, corresponding to the primal variables $x$ . The associated Lagrangian dual problem is given by</p>
<script type="math/tex; mode=display">
\underset{\lambda\in\mathbb R^m}{\max}{\mathfrak D(\lambda)}\\
\text{subject to}\quad\lambda\ge0</script><p>where $λ$ are the dual variables and $\mathfrak D(\lambda)=min_{x\in \mathbb R^D}\mathfrak L(x,\lambda)$.</p>
</li>
<li><p><strong><em>The minimax inequality</em></strong>, which says that for any function with two arguments $φ(x, y)$, <em>the maximin is less than the minimax</em>, i.e.,</p>
<script type="math/tex; mode=display">
\underset{y}{\max}\underset{x}{\min}\varphi(x,y)\le\underset{x}{\min}\underset{y}{\max}\varphi(x,y)</script><p>This inequality can be proved by considering the inequality</p>
<script type="math/tex; mode=display">
\text{For all}\quad x,y\quad
\underset{x}{\min}\varphi(x,y)\le\underset{y}{\max}\varphi(x,y)</script><p>Note that taking the maximum over $y$ of the left-hand side maintains the inequality since the inequality is true for all $y$. Similarly, we can take the minimum over $x$ of the right-hand side to obtain the minimax inequality.</p>
</li>
<li><p>The second concept is <strong><em>weak duality</em></strong>, which uses the <strong><em>minimax inequality</em></strong>to show that <em>primal values are always greater than or equal to dual values</em>. </p>
</li>
<li><p>When $λ ⩾ 0$, the Lagrangian $L(x, λ)$ is a lower bound of $J(x)$. Hence, the maximum of $L(x, λ)$ with respect to $λ$ is</p>
<script type="math/tex; mode=display">
J(x)=\underset{\lambda\ge0}{\max}\mathfrak L(x,\lambda)</script><p>Recall that the original problem was minimizing $J(x)$,</p>
<script type="math/tex; mode=display">
\underset{x\in\mathbb R^D}{\min}\underset{\lambda\ge0}{\max}\mathfrak L(x,\lambda)</script><p>By the minimax inequality, it follows that swapping the order of the minimum and maximum results in a <em>smaller</em> value, i.e.,</p>
<script type="math/tex; mode=display">
\underset{x\in\mathbb R^D}{\min}\underset{\lambda\ge0}{\max}\mathfrak L(x,\lambda)\ge\underset{\lambda\ge0}{\max}\underset{x\in\mathbb R^D}{\min}\mathfrak L(x,\lambda)</script><p>This is also known as <strong><em>weak duality</em></strong>. Note that the inner part of the right-hand side is the dual objective function $\mathfrak D(λ)$.</p>
</li>
<li><p>In contrast to the original optimization problem, which has constraints,$\min_{x∈\mathbb R^d}\mathfrak L(x, λ)$ is an unconstrained optimization problem for a given value of $λ$. If solving $\min_{x∈\mathbb R^d}\mathfrak L(x, λ)$ is easy, then the overall problem is easy to solve. We can see that $\mathfrak L(x, λ)$ is affine with respect to $λ$. Therefore $\min_{x∈\mathbb R^d}\mathfrak L(x, λ)$ is a pointwise minimum of affine functions of $λ$, and hence $\mathfrak D(λ)$ is concave even though $f(·)$ and $g_i(·)$ may be nonconvex. The outer problem, maximization over $λ$, is the maximum of a concave function and can be efficiently computed. <strong>Assuming $f(·)$ and $g_i(·)$ are differentiable, we find the Lagrange dual problem by differentiating the Lagrangian with respect to $x$, setting the differential to zero, and solving for the optimal value.</strong></p>
</li>
<li><p><strong>Equality Constraints</strong>: Consider  with additional equality constraints</p>
<script type="math/tex; mode=display">
\begin{split}
\underset{x}{\min}\quad&f(x)\\
\text{subject to}\quad &g_i(x)\le0\quad\text{for all}\quad i=1,...,m\\
&h_j(x)=0\quad\text{for all}\quad j=1,...,n
\end{split}</script><p>We can model equality constraints by replacing them with two inequality constraints. That is for each equality constraint $h_j(x) = 0$ we equivalently replace it by two constraints $h_j(x) ⩽ 0$ and $h_j(x) ⩾ 0$​. It turns out that the resulting Lagrange multipliers are then unconstrained.Therefore, we constrain the Lagrange multipliers corresponding to the<br>inequality constraints to be non-negative, and leave the Lagrange multipliers corresponding to the equality constraints unconstrained.</p>
</li>
</ul>
<h2 id="Convex-Optimization"><a href="#Convex-Optimization" class="headerlink" title="Convex Optimization"></a>Convex Optimization</h2><ul>
<li><p>When $f(·)$ is a convex function, and when the constraints involving $g(·)$ and $h(·)$ are convex sets, this is called a <em>convex optimization problem</em>. In this setting, we have <em>strong duality</em>: The optimal solution of the dual problem is the same as the optimal solution of the primal problem.</p>
</li>
<li><p>A set $\mathcal C$ is a <strong><em>convex set</em></strong> if for any $x, y ∈\mathcal C$ and for any scalar $θ$ with $0 ⩽ θ ⩽ 1$, we have</p>
<script type="math/tex; mode=display">
\theta x+(1-\theta)y\in\mathcal C</script><p>Convex sets are sets such that <strong>a straight line connecting any two elements of the set lie inside the set</strong>.</p>
<p><strong>Convex functions are functions such that a straight line between anytwo points of the function lie above the function.</strong></p>
<p><img src="https://cdn.jsdelivr.net/gh/Eliauk-L/blog-img@img/img/202408061138434.png" alt="Example of a convex set"></p>
</li>
<li><p>Let function $f :\mathbb R^D →\mathbb R$ be a function whose domain is a convex set. The function $f$ is a <strong><em>convex function</em></strong> if for all $x, y$ in the domain of $f$, and for any scalar $θ$ with $0 ⩽ θ ⩽ 1$, we have</p>
<script type="math/tex; mode=display">
f(\theta x+(1-\theta)y)\le\theta f(x)+(1-\theta)f(y)</script><p><em>A concave function is the negative of a convex function.</em></p>
<p>Another relation between convex functions and convex sets is to consider the set obtained by “filling in” a convex function. A convex function is a bowl-like object, and we imagine pouring water into it to fill it up. This resulting filled-in set, called the <em>epigraph</em> of the convex function, is a convex set.</p>
</li>
<li><p>If a function $f :\mathbb R^n →\mathbb R$ is differentiable, we can specify convexity in terms of its gradient $∇_xf(x)$. A function $f(x)$ is convex if and only if for any two points $x, y$ it holds that</p>
<script type="math/tex; mode=display">
f(y) ⩾ f(x) +∇_xf(x)^T(y − x)</script><p>If we further know that a function $f(x)$ is twice differentiable, that is, the Hessian exists for all values in the domain of $x$, then the function $f(x)$ is convex if and only if $∇^2<br>_xf(x)$ is positive semidefinite.</p>
</li>
<li><p>A nonnegative weighted sum of convex functions is convex.</p>
</li>
<li><p>If $f_1$ and $f_2$ are convex functions, then we have by the definition</p>
<script type="math/tex; mode=display">
f_1(\theta x+(1-\theta)y)\le\theta f_1(x)+(1-\theta)f_1(y)\\
f_2(\theta x+(1-\theta)y)\le\theta f_2(x)+(1-\theta)f_2(y)</script><p>Summing up both sides gives us</p>
<script type="math/tex; mode=display">
f_1(\theta x+(1-\theta)y)+f_1(\theta x+(1-\theta)y)\le\\\theta f_1(x)+(1-\theta)f_1(y)+\theta f_2(x)+(1-\theta)f_2(y)</script><p>where the right-hand side can be rearranged to</p>
<script type="math/tex; mode=display">
\theta(f_1(x)+f_2(x))+(1-\theta)(f_1(y)+f_2(y))</script></li>
<li><p>In summary, a constrained optimization problem is called a convex optimization problem if</p>
<script type="math/tex; mode=display">
\begin{split}
\underset{x}{\min}\quad&f(x)\\
\text{subject to}\quad &g_i(x)\le0\quad\text{for all}\quad i=1,...,m\\
&h_j(x)=0\quad\text{for all}\quad j=1,...,n
\end{split}</script><p>where all functions $f(x)$ and $g_i(x)$ are convex functions, and all $h_j(x) = 0$ are convex sets.</p>
</li>
<li><p><strong>Linear Programming</strong>:  Consider the special case when all the preceding functions are linear, i.e.,</p>
<script type="math/tex; mode=display">
\begin{split}
\underset{x\in\mathbb R^d}{\min}\quad&c^Tx\\
\text{subject to}\quad &Ax\le b
\end{split}</script><p>where $A ∈\mathbb R^{m×d}$ and $b ∈\mathbb R^m$. This is known as a <strong><em>linear program</em></strong>. It has $d$ variables and $m$ linear constraints. The Lagrangian is given by</p>
<script type="math/tex; mode=display">
\mathfrak L(x,\lambda)=c^Tx+\lambda^T(Ax-b)</script><p>where $λ ∈\mathbb R^m$ is the vector of non-negative Lagrange multipliers. Rearranging the terms corresponding to $x$ yields</p>
<script type="math/tex; mode=display">
\mathfrak L(x,\lambda)=(c+A^T\lambda)^Tx-\lambda^Tb</script><p>Taking the derivative of $\mathfrak L(x, λ)$ with respect to $x$ and setting it to zero gives us</p>
<script type="math/tex; mode=display">
c+A^T\lambda=0</script><p>Therefore, the dual Lagrangian is $\mathfrak D(λ) = −λ^⊤b$. Recall we would like to maximize $D(λ)$. In addition to the constraint due to the derivative of $L(x, λ)$ being zero, we also have the fact that $λ ⩾ 0$, resulting in the following dual optimization problem</p>
<script type="math/tex; mode=display">
\begin{split}
\underset{\lambda\in\mathbb R^m}{\max}\quad&-b^T\lambda\\
\text{subject to}\quad &c+A^T\lambda=0\\
&\lambda \ge 0
\end{split}</script><p>This is also a <strong><em>linear program</em></strong>, but with $m$ variables. We have the choice of solving the primal or the dual program depending on whether $m$ or $d$ is larger. Recall that $d$ is the number of variables and $m$ is the number of constraints in the primal linear program.</p>
</li>
<li><p><strong>Quadratic Programming</strong>:  Consider the case of a convex quadratic objective function, where the constraints are affine, i.e.</p>
<script type="math/tex; mode=display">
\begin{split}
\underset{x\in\mathbb R^d}{\min}\quad&{\frac{1}{2}x^TQx+c^Tx}\\
\text{subject to}\quad &Ax\le b
\end{split}</script><p>where $A ∈\mathbb R^{m×d},b\in\mathbb R^m$ and $c ∈\mathbb R^d$. The square symmetric matrix $Q ∈\mathbb R^{d×d}$ is positive definite, and therefore the objective function is convex.T his is known as a <strong><em>quadratic program</em></strong>. Observe that it has $d$ variables and $m$ linear constraints.</p>
<p>The Lagrangian is given by</p>
<script type="math/tex; mode=display">
\begin{split}
\mathfrak L(x,\lambda)&={\frac{1}{2}x^TQx+c^Tx}+\lambda^T(Ax-b)\\
&=\frac{1}{2}x^TQx+(c+A^T\lambda)^Tx-\lambda^Tb
\end{split}</script><p>where again we have rearranged the terms. Taking the derivative of $\mathfrak D(x, λ)$ with respect to $x$ and setting it to zero gives</p>
<script type="math/tex; mode=display">
Qx+(c+A^T\lambda)=0</script><p>Assuming that $Q$ is invertible, we get</p>
<script type="math/tex; mode=display">
x=-Q^{-1}(c+A^T\lambda)</script><p>Substituting $x$ into the primal Lagrangian $\mathfrak L(x, λ)$, we get the dual Lagrangian</p>
<script type="math/tex; mode=display">
\mathfrak D(\lambda)=-\frac{1}{2}(c+A^T\lambda)^TQ^{-1}(c+A^T\lambda)-\lambda^Tb</script><p>Therefore, the dual optimization problem is given by</p>
<script type="math/tex; mode=display">
\begin{split}
\underset{\lambda\in\mathbb R^m}{\max}\quad&{-\frac{1}{2}(c+A^T\lambda)^TQ^{-1}(c+A^T\lambda)-\lambda^Tb}\\
\text{subject to}\quad&\lambda \ge 0
\end{split}</script></li>
<li><p>One useful fact about a convex set is that it can be equivalently described by its supporting hyperplanes. A hyperplane is called a <strong><em>supporting hyperplane</em></strong> of a convex set if it intersects the convex set, and the convex set is contained on just one side of it. Recall that we can fill up a convex function to obtain the epigraph, which is a convex set. Therefore,<br>we can also describe convex functions in terms of their supporting hyperplanes. Furthermore, observe that the supporting hyperplane just touches the convex function, and is in fact the tangent  $\frac{df(x)}{dx}|_{x=x_0}$ to the function at that point.</p>
</li>
<li><p>The <strong><em>Legendre-Fenchel transform</em></strong> is a transformation (in the sense of a Fourier transform) from a convex differentiable function $f(x)$ to a function that depends on the tangents $s(x) = ∇_xf(x)$. It is worth stressing that this is a transformation of the function $f(·)$ and not the variable $x$ or the function evaluated at $x$. The Legendre-Fenchel transform is also known as the <strong><em>convex conjugate</em></strong>  and is closely related to duality.</p>
</li>
<li><p>The <strong><em>convex conjugate</em></strong> of a function $f :\mathbb R^D →\mathbb R$ is a function $f^∗$ defined by</p>
<script type="math/tex; mode=display">
f^*(s)=\underset{x\in\mathbb R^D}{\text{sup}}(\left<s,x\right>-f(x))</script><p>Note that the preceding convex conjugate definition does not need the function f to be convex nor differentiable. $\left<s,x\right>=s^Tx$</s,x\right></p>
</li>
<li><p>For a convex differentiable function, we know that at $x_0$ the tangent touches $f(x_0)$ so that</p>
<script type="math/tex; mode=display">
f(x_0)=sx_0+c</script><p>Recall that we want to describe the convex function $f(x)$ in terms of its gradient $∇_xf(x)$, and that $s = ∇_xf(x_0)$. We rearrange to get an expression for $−c$ to obtain</p>
<script type="math/tex; mode=display">
-c=sx_0-f(x_0)</script><p>Note that $−c$ changes with $x_0$ and therefore with $s$, which is why we can think of it as a function of $s$, which we call</p>
<script type="math/tex; mode=display">
f^*(s):=sx_0-f(x_0)</script></li>
<li><p>For convex functions, applying the Legendre transform again gets us back to the original function.</p>
<blockquote>
<p>e.g. <strong>Convex Conjugates</strong><br>To illustrate the application of convex conjugates, consider the quadratic function</p>
<script type="math/tex; mode=display">
f(y)=\frac{\lambda}{2}y^TK^{-1}y</script><p>based on a positive definite matrix $K ∈\mathbb R^{n×n}$. We denote the primal variable to be $y ∈\mathbb R^n$ and the dual variable to be $α ∈\mathbb R^n$.</p>
<p>Applying definition, we obtain the function</p>
<script type="math/tex; mode=display">
f^*(\alpha)=\underset{y\in\mathbb R^n}{\text{sup}}\left<y,\alpha\right>-\frac{\lambda}{2}y^TK^{-1}y</script><p>Since the function is differentiable, we can find the maximum by taking the derivative and with respect to $y$ setting it to zero</p>
<script type="math/tex; mode=display">
\frac{\partial\left[\left<y,\alpha\right>-\frac{\lambda}{2}y^TK^{-1}y\right]}{\partial y}=(\alpha-\lambda K^{-1}y)^T</script><p>and hence when the gradient is zero we have $y =\frac{1}{λ}Kα$. Substituting into yields</p>
<script type="math/tex; mode=display">
f^*(\alpha)=\frac{1}{\lambda}\alpha^TK\alpha-\frac{\lambda}{2}\left(\frac{1}{\lambda}K\alpha\right)^TK^{-1}(\frac{1}{\lambda}K\alpha)=\frac{1}{2\lambda}\alpha^TK\alpha</script><p>e.g. we derive the convex conjugate of a sum of losses $ℓ(t)$, where $ℓ :\mathbb R →\mathbb R$. This also illustrates the application of the convex conjugate to the vector case. Let $L(t) =\sum_{i=1}^nℓ_i(t_i)$.</p>
<script type="math/tex; mode=display">
\begin{split}
\mathcal L^*(z)&=\underset{t\in\mathbb R^n}{\text{sup}}\left<z,t\right>-\sum_{i=1}^nℓ_i(t_i)\\
&=\underset{t\in\mathbb R^n}{\text{sup}}\sum_{i=1}^nz_it_i-ℓ_i(t_i)\\
&=\sum_{i=1}^n\underset{t\in\mathbb R^n}{\text{sup}}z_it_i-ℓ_i(t_i)\\
&=\sum_{i=1}^nℓ^*_i(z_i)
\end{split}</script><p>e.g.  The Legendre-Fenchel transform described here also can be used to derive a dual optimization problem.</p>
<p>Let $f(y)$ and $g(x)$ be convex functions, and $A$ a real matrix of appropriate dimensions such that $Ax = y$. Then</p>
<script type="math/tex; mode=display">
\underset{x}{\min}f(Ax)+g(x)=\underset{Ax=y}{\min}f(y)+g(x)</script><p>By introducing the Lagrange multiplier $u$ for the constraints $Ax = y$,</p>
<script type="math/tex; mode=display">
\begin{split}
\underset{Ax=y}{\min}f(y)+g(x)&=\underset{x,y}{\min}\underset{u}{\max}f(y)+g(x)+(Ax-y)^Tu\\
&=\underset{u}{\max}\underset{x,y}{\min}f(y)+g(x)+(Ax-y)^Tu\\
\end{split}</script><p>where the last step of swapping max and min is due to the fact that $f(y)$ and $g(x)$ are convex functions. By splitting up the dot product term and collecting $x$ and $y$,</p>
<script type="math/tex; mode=display">
\begin{split}
&\underset{u}{\max}\underset{x,y}{\min}f(y)+g(x)+(Ax-y)^Tu\\
=&\underset{u}{\max}\left[\underset{y}{\min}-y^Tu+f(y)\right]+\left[\underset{x}{\min}(Ax)^Tu+g(x)\right]\\
=&\underset{u}{\max}\left[\underset{y}{\min}-y^Tu+f(y)\right]+\left[\underset{x}{\min}x^TA^Tu+g(x)\right]\\
\end{split}</script><p>Recall the convex conjugate and the fact that dot products are symmetric,</p>
<script type="math/tex; mode=display">
\begin{split}
&\underset{u}{\max}\left[\underset{y}{\min}-y^Tu+f(y)\right]+\left[\underset{x}{\min}x^TA^Tu+g(x)\right]\\
=&\underset{u}{\max}-f^*(u)-g^*(-A^Tu)
\end{split}</script><p>Therefore, we have shown that</p>
<script type="math/tex; mode=display">
\underset{x}{\min}f(Ax)+g(x)=\underset{u}{\max}-f^*(u)-g^*(-A^Tu)</script></blockquote>
</li>
</ul>
<blockquote>
<p><strong>Bibliography:</strong></p>
<ol>
<li>Mathematics for Machine Learning_Marc Peter Deisenroth_2020</li>
</ol>
</blockquote>

                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        文章作者:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/about" rel="external nofollow noreferrer">Jay</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        文章链接:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="http://Eliauk-L.github.io/2024/08/06/continuousoptimization/">http://Eliauk-L.github.io/2024/08/06/continuousoptimization/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        版权声明:
                    </i>
                </span>
                <span class="reprint-info">
                    本博客所有文章除特別声明外，均采用
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    许可协议。转载请注明来源
                    <a href="/about" target="_blank">Jay</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>复制成功，请遵循本文的转载规则</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">查看</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/tags/Mathematics/">
                                    <span class="chip bg-color">Mathematics</span>
                                </a>
                            
                                <a href="/tags/%E7%AC%94%E8%AE%B0/">
                                    <span class="chip bg-color">笔记</span>
                                </a>
                            
                                <a href="/tags/AnalyticGeometry/">
                                    <span class="chip bg-color">AnalyticGeometry</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="qq, qzone, wechat, weibo, douban" data-wechat-qrcode-helper="<p>微信扫一扫即可分享！</p>"></div>
    <script src="/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;上一篇</div>
            <div class="card">
                <a href="/2024/08/07/whenmodelsmeetdata/">
                    <div class="card-image">
                        
                        
                        <img src="/medias/featureimages/3.jpg" class="responsive-img" alt="When Models Meet Data">
                        
                        <span class="card-title">When Models Meet Data</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2024-08-07
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/categories/MachineLearing/" class="post-category">
                                    MachineLearing
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/tags/Mathematics/">
                        <span class="chip bg-color">Mathematics</span>
                    </a>
                    
                    <a href="/tags/MachineLearing/">
                        <span class="chip bg-color">MachineLearing</span>
                    </a>
                    
                    <a href="/tags/%E7%AC%94%E8%AE%B0/">
                        <span class="chip bg-color">笔记</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                下一篇&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/2024/08/05/jdbc-si-wei-dao-tu/">
                    <div class="card-image">
                        
                        
                        <img src="/medias/featureimages/44.jpg" class="responsive-img" alt="JDBC思维导图">
                        
                        <span class="card-title">JDBC思维导图</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2024-08-05
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/categories/Java/" class="post-category">
                                    Java
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/tags/%E7%AC%94%E8%AE%B0/">
                        <span class="chip bg-color">笔记</span>
                    </a>
                    
                    <a href="/tags/Java/">
                        <span class="chip bg-color">Java</span>
                    </a>
                    
                    <a href="/tags/%E6%95%B0%E6%8D%AE%E5%BA%93/">
                        <span class="chip bg-color">数据库</span>
                    </a>
                    
                    <a href="/tags/JDBC/">
                        <span class="chip bg-color">JDBC</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- 代码块功能依赖 -->
<script type="text/javascript" src="/libs/codeBlock/codeBlockFuction.js"></script>


  <!-- 是否加载使用自带的 prismjs. -->
  <script type="text/javascript" src="/libs/prism/prism.min.js"></script>


<!-- 代码语言 -->

<script type="text/javascript" src="/libs/codeBlock/codeLang.js"></script>


<!-- 代码块复制 -->

<script type="text/javascript" src="/libs/codeBlock/codeCopy.js"></script>


<!-- 代码块收缩 -->

<script type="text/javascript" src="/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;目录</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC 悬浮按钮. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* 修复文章卡片 div 的宽度. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // 切换TOC目录展开收缩的相关操作.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>

    

</main>


<script src="https://cdn.bootcss.com/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
<script>
    MathJax.Hub.Config({
        tex2jax: {inlineMath: [['$', '$'], ['\\(', '\\)']]}
    });
</script>



    <footer class="page-footer bg-color">
    

    <div class="container row center-align"
         style="margin-bottom: 0px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024</span>
            
            <a href="/about" target="_blank">Jay</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;总访问量:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;总访问人数:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- 运行天数提醒. -->
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Eliauk-L" class="tooltipped" target="_blank" data-tooltip="访问我的GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:2571368706@qq.com" class="tooltipped" target="_blank" data-tooltip="邮件联系我" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>







    <a href="tencent://AddContact/?fromId=50&fromSubId=1&subcmd=all&uin=2571368706" class="tooltipped" target="_blank" data-tooltip="QQ联系我: 2571368706" data-position="top" data-delay="50">
        <i class="fab fa-qq"></i>
    </a>







</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- 搜索遮罩框 -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;搜索</span>
            <input type="search" id="searchInput" name="s" placeholder="请输入搜索的关键字"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- 白天和黑夜主题 -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- 回到顶部按钮 -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/libs/materialize/materialize.min.js"></script>
    <script src="/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/libs/aos/aos.js"></script>
    <script src="/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/js/matery.js"></script>

    

    
    
    

    <!-- 雪花特效 -->
     
        <script type="text/javascript">
            // 只在桌面版网页启用特效
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/libs/others/snow.js"><\/script>');
            }
        </script>
    

    <!-- 鼠标星星特效 -->
    

    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--腾讯兔小巢-->
    
    

    

    

    
    <script src="/libs/instantpage/instantpage.js" type="module"></script>
    

</body>

</html>
